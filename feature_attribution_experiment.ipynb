{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaNWRhlFHpi1WmD+xj3da+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kristina-26/LLM-interpretability/blob/main/feature_attribution_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6z4wgfSGVN4"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers==4.44.2 captum==0.7.0 torch==2.3.1 scikit-learn==1.5.1 matplotlib==3.9.0 seaborn==0.13.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, List, Tuple\n",
        "import torch.nn as nn\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "\n",
        "from captum.attr import (\n",
        "    IntegratedGradients,\n",
        "    LayerIntegratedGradients,\n",
        "    TokenReferenceBase,\n",
        "    Saliency,\n",
        "    visualization\n",
        ")"
      ],
      "metadata": {
        "id": "BtO3qLzXGZhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Controlled experiment with multinomial logistic regression + coefficient heatmap"
      ],
      "metadata": {
        "id": "pYAHIfX-N8R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Heatmaps method\n",
        "\n",
        "Mi,c = wi,c * xi"
      ],
      "metadata": {
        "id": "NfHSytt6IviQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "n_classes = 3\n",
        "n_per = 6\n",
        "n = n_classes * n_per  # 18 samples\n",
        "m = 9                 # 9 features\n",
        "X = np.random.randn(n, m)\n",
        "y = np.repeat(np.arange(n_classes), n_per)\n",
        "\n",
        "# add signal to features [0:3] -> class0, [3:6] -> class1, [6:9] -> class2\n",
        "X[y==0, :3]  += 3\n",
        "X[y==1, 3:6] += 3\n",
        "X[y==2, 6:]  += 3\n",
        "\n",
        "# add two noisy labels\n",
        "y_noisy = y.copy()\n",
        "y_noisy[[2, 13]] = (y_noisy[[2, 13]] + 1) % n_classes\n",
        "\n",
        "class MultinomialLogReg(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# convert to PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X)\n",
        "y_tensor = torch.LongTensor(y_noisy)\n",
        "\n",
        "model = MultinomialLogReg(m, n_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.LBFGS(model.parameters(), lr=0.1)\n",
        "\n",
        "# training loop\n",
        "def closure():\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_tensor)\n",
        "    loss = criterion(outputs, y_tensor)\n",
        "    loss.backward()\n",
        "    return loss\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.step(closure)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_tensor)\n",
        "    _, y_pred_torch = torch.max(outputs, 1)\n",
        "    acc_torch = accuracy_score(y_noisy, y_pred_torch.numpy())\n",
        "\n",
        "print(f\"PyTorch model accuracy: {acc_torch:.3f}\")\n",
        "\n",
        "# coefficients\n",
        "coefs = model.linear.weight.data.numpy()\n",
        "\n",
        "# feature importance: Mi,c = wi,c * xi, average for class level importance\n",
        "feature_importance = np.zeros((n_classes, m))\n",
        "for class_idx in range(n_classes):\n",
        "    class_mask = (y_noisy == class_idx)\n",
        "    if class_mask.sum() > 0:\n",
        "        # for each sample in this class find Mi,c = wi,c * xi, then average\n",
        "        class_samples = X[class_mask]\n",
        "        for i, sample in enumerate(class_samples):\n",
        "            Mi_c = coefs[class_idx] * sample\n",
        "            feature_importance[class_idx] += Mi_c\n",
        "        feature_importance[class_idx] /= len(class_samples)  # average\n",
        "\n",
        "for i, row in enumerate(feature_importance):\n",
        "    print(f\"Class {i}: {' '.join([f'{x:6.3f}' for x in row])}\")"
      ],
      "metadata": {
        "id": "Mxo7fwOTG9NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Integrated gradients"
      ],
      "metadata": {
        "id": "ZcpCQyZcOA9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ig = IntegratedGradients(model)\n",
        "\n",
        "# attributions for each sample\n",
        "all_attributions = []\n",
        "baseline = torch.zeros_like(X_tensor[0:1])  # zero baseline\n",
        "\n",
        "for i in range(n):\n",
        "    input_sample = X_tensor[i:i+1]\n",
        "    target_class = int(y_noisy[i])\n",
        "\n",
        "    # attribution\n",
        "    attribution = ig.attribute(input_sample, baseline, target=target_class, n_steps=50)\n",
        "    all_attributions.append(attribution.squeeze().detach().numpy())\n",
        "\n",
        "attributions = np.array(all_attributions)\n",
        "\n",
        "# average attribution across samples of same class\n",
        "class_attributions = np.zeros((n_classes, m))\n",
        "for class_idx in range(n_classes):\n",
        "    mask = (y_noisy == class_idx)\n",
        "    if mask.sum() > 0:\n",
        "        class_attributions[class_idx] = attributions[mask].mean(axis=0)"
      ],
      "metadata": {
        "id": "W7pmsUzdN-DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.heatmap(coefs, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True,\n",
        "            yticklabels=[f\"class_{i}\" for i in range(n_classes)],\n",
        "            xticklabels=[f\"x{j}\" for j in range(m)],\n",
        "            center=0)\n",
        "plt.title(\"Coefficients (wi,c)\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.heatmap(feature_importance, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar=True,\n",
        "            yticklabels=[f\"class_{i}\" for i in range(n_classes)],\n",
        "            xticklabels=[f\"x{j}\" for j in range(m)],\n",
        "            center=0)\n",
        "plt.title(\"Method 1: Feature importance\\n(Mi,c = wi,c * xi)\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.heatmap(class_attributions, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar=True,\n",
        "            yticklabels=[f\"class_{i}\" for i in range(n_classes)],\n",
        "            xticklabels=[f\"x{j}\" for j in range(m)],\n",
        "            center=0)\n",
        "plt.title(\"Method 2: Integrated gradients\\n(avg attribution per class)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pleYGpFIVgd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Extract Coefficients\n",
        "coefs = model.linear.weight.data.numpy()\n",
        "\n",
        "# 2. Calculate Method 1: Analytical Feature Importance (Weight * Input)\n",
        "feature_importance = np.zeros((n_classes, m))\n",
        "for class_idx in range(n_classes):\n",
        "    class_mask = (y_noisy == class_idx)\n",
        "    if class_mask.sum() > 0:\n",
        "        class_samples = X[class_mask]\n",
        "        # Broadcasting: (n_samples, n_features) * (n_features,)\n",
        "        Mi_c = class_samples * coefs[class_idx]\n",
        "        feature_importance[class_idx] = Mi_c.mean(axis=0)\n",
        "\n",
        "# 3. Calculate Method 2: Integrated Gradients\n",
        "ig = IntegratedGradients(model)\n",
        "baseline = torch.zeros_like(X_tensor[0:1]) # zero baseline\n",
        "attributions_list = []\n",
        "\n",
        "for i in range(n):\n",
        "    input_sample = X_tensor[i:i+1]\n",
        "    target_class = int(y_noisy[i])\n",
        "    attr = ig.attribute(input_sample, baseline, target=target_class, n_steps=50)\n",
        "    attributions_list.append(attr.detach().numpy().squeeze())\n",
        "\n",
        "attributions = np.array(attributions_list)\n",
        "\n",
        "# Average IG attributions per class\n",
        "class_attributions = np.zeros((n_classes, m))\n",
        "for class_idx in range(n_classes):\n",
        "    mask = (y_noisy == class_idx)\n",
        "    if mask.sum() > 0:\n",
        "        class_attributions[class_idx] = attributions[mask].mean(axis=0)\n",
        "\n",
        "# --- VERIFICATION (Addresses Redundancy) ---\n",
        "# Prove that Method 1 and Method 2 are nearly identical\n",
        "diff = np.abs(feature_importance - class_attributions).max()\n",
        "print(f\"Max difference between Analytical and IG importance: {diff:.6f}\")\n",
        "print(\"(This confirms that for Linear Models, IG == Weight * Input. We only need to plot one.)\")\n",
        "\n",
        "# --- PLOTTING ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Model Coefficients (The \"Global\" Logic)\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(coefs, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True,\n",
        "            yticklabels=[f\"Class {i}\" for i in range(n_classes)],\n",
        "            xticklabels=[f\"x{j}\" for j in range(m)],\n",
        "            center=0, annot_kws={\"size\": 10}) # Adjusted font size\n",
        "plt.title(\"Model coefficients (weights)\", fontsize=12, pad=10)\n",
        "\n",
        "# Plot 2: Feature Attribution (The \"Local\" Logic)\n",
        "# We plot the IG results, but title clarifies it matches analytical importance\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(class_attributions, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True,\n",
        "            yticklabels=[f\"Class {i}\" for i in range(n_classes)],\n",
        "            xticklabels=[f\"x{j}\" for j in range(m)],\n",
        "            center=0, annot_kws={\"size\": 10})\n",
        "plt.title(\"Feature importance (Method 1)\\n and Integrated Gradients (Method 2)\", fontsize=12, pad=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9WKJxLw_CeSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison of feature importance and integrated gradients\n",
        "\n",
        "both methods give almost identical results when averaged across classes. This makes sense because for linear models (like logistic regression), integrated gradients should converge to the same result as feature importance"
      ],
      "metadata": {
        "id": "5pamuddIXVaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Coefficients (wi,c):\")\n",
        "for i, row in enumerate(coefs):\n",
        "    print(f\"Class {i}: {' '.join([f'{x:6.2f}' for x in row])}\")\n",
        "\n",
        "print(\"\\n1. Feature importance (Mi,c = wi,c × xi):\")\n",
        "for i, row in enumerate(feature_importance):\n",
        "    print(f\"Class {i}: {' '.join([f'{x:6.9f}' for x in row])}\")\n",
        "\n",
        "print(\"\\n2. Integrated gradients:\")\n",
        "for i, row in enumerate(class_attributions):\n",
        "    print(f\"Class {i}: {' '.join([f'{x:6.9f}' for x in row])}\")\n",
        "\n",
        "# Compute correlation between the two interpretability methods\n",
        "correlation_matrix = np.corrcoef(feature_importance.flatten(), class_attributions.flatten())[0,1]\n",
        "print(f\"\\nCorrelation between feature importance and IG: {correlation_matrix:.20f}\")"
      ],
      "metadata": {
        "id": "rhNt8GrVWca6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concrete examples with feature importance and IG"
      ],
      "metadata": {
        "id": "HZvIY3tbitVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_indices = [0, 6, 12]  # one from each class\n",
        "for idx in sample_indices:\n",
        "    true_class = y[idx]\n",
        "    noisy_class = y_noisy[idx]\n",
        "    predicted_class = y_pred_torch[idx].item()\n",
        "\n",
        "    print(f\"\\nSample {idx}: True={true_class}, Noisy={noisy_class}, Predicted={predicted_class}\")\n",
        "    print(f\"Feature values: {X[idx]}\")\n",
        "\n",
        "    # Method 1: feature importance Mi,c = wi,c × xi\n",
        "    weights_for_predicted = coefs[predicted_class]\n",
        "    sample_feature_importance = weights_for_predicted * X[idx]\n",
        "\n",
        "    print(f\"Method 1 (feature importance): {sample_feature_importance}\")\n",
        "    print(f\"Method 2 (integrated gradients):  {attributions[idx]}\")\n",
        "\n",
        "    # which features each method identifies as most important\n",
        "    fi_top3 = np.argsort(np.abs(sample_feature_importance))[-3:]\n",
        "    ig_top3 = np.argsort(np.abs(attributions[idx]))[-3:]\n",
        "\n",
        "    # important features based on ground truth\n",
        "    expected_important = []\n",
        "    if true_class == 0: expected_important = [0,1,2]\n",
        "    elif true_class == 1: expected_important = [3,4,5]\n",
        "    elif true_class == 2: expected_important = [6,7,8]\n",
        "\n",
        "    print(f\"Expected important features:     {expected_important}\")\n",
        "    print(f\"Method 1 top 3 features:         {fi_top3}\")\n",
        "    print(f\"Method 2 top 3 features:         {ig_top3}\")\n",
        "\n",
        "    # agreement between methods\n",
        "    agreement = len(set(fi_top3) & set(ig_top3))\n",
        "    print(f\"Agreement (features in both top-3): {agreement}/3\")\n",
        "\n",
        "    # correlation for this sample\n",
        "    sample_correlation = np.corrcoef(sample_feature_importance, attributions[idx])[0,1]\n",
        "    print(f\"Sample correlation: {sample_correlation:.6f}\")"
      ],
      "metadata": {
        "id": "AfYOBFcdYB3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature attribution of text classification"
      ],
      "metadata": {
        "id": "KHxToOLss9wA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classical baseline models"
      ],
      "metadata": {
        "id": "I_bQlS0EoDHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading"
      ],
      "metadata": {
        "id": "AdW4_S_FoaR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_from_upload() -> pd.DataFrame:\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise RuntimeError(\"no file uploaded.\")\n",
        "    fname = list(uploaded.keys())[0]\n",
        "    print(f\"file uploaded: {fname}\")\n",
        "    df = pd.read_csv(io.BytesIO(uploaded[fname]))\n",
        "    # normalize column names\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    if 'text' in cols and cols['text'] != 'text':\n",
        "        df = df.rename(columns={cols['text']: 'text'})\n",
        "    if 'label' in cols and cols['label'] != 'label':\n",
        "        df = df.rename(columns={cols['label']: 'label'})\n",
        "    assert 'text' in df.columns and 'label' in df.columns\n",
        "    df = df[['text','label']].dropna().reset_index(drop=True)\n",
        "    print(f\"loaded {len(df)} samples\")\n",
        "    print(\"class distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    return df\n",
        "\n",
        "df = load_dataset_from_upload()\n",
        "df.head()\n",
        "\n",
        "# encode labels + stratified split\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['text'], df['label_encoded'], test_size=0.2, stratify=df['label_encoded'], random_state=42\n",
        ")\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f\"num classes: {num_classes}, classes: {list(label_encoder.classes_)}\")"
      ],
      "metadata": {
        "id": "VTemsBeuoZ3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_ci(y_true, y_pred, metric_fn, B: int = 1000, seed: int = 42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(len(y_true))\n",
        "    scores = []\n",
        "    for _ in range(B):\n",
        "        s = rng.choice(idx, size=len(idx), replace=True)\n",
        "        scores.append(metric_fn(np.array(y_true)[s], np.array(y_pred)[s]))\n",
        "    lo, med, hi = np.percentile(scores, [2.5, 50, 97.5])\n",
        "    return lo, med, hi\n",
        "\n",
        "def print_ci(name: str, lo: float, med: float, hi: float):\n",
        "    print(f\"{name}: {med:.3f}  (95% ci: {lo:.3f} … {hi:.3f})\")"
      ],
      "metadata": {
        "id": "5Qlrt2jSYQvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classical models: tf-idf + logistic regression, tf-idf + linear svm"
      ],
      "metadata": {
        "id": "kUn3s--CoRzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# classical baselines: tf-idf + logistic regression, linear svm (with bootstrap cis)\n",
        "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "Xtr_vec = vectorizer.fit_transform(X_train)\n",
        "Xva_vec = vectorizer.transform(X_val)\n",
        "\n",
        "# logistic regression (multinomial)\n",
        "logreg = LogisticRegression(max_iter=2000, multi_class='multinomial')\n",
        "logreg.fit(Xtr_vec, y_train)\n",
        "yhat_log = logreg.predict(Xva_vec)\n",
        "\n",
        "# linear svm\n",
        "linsvm = LinearSVC()\n",
        "linsvm.fit(Xtr_vec, y_train)\n",
        "yhat_svm = linsvm.predict(Xva_vec)\n",
        "\n",
        "# metrics + cis\n",
        "for name, yhat in [(\"logreg\", yhat_log), (\"linear_svm\", yhat_svm)]:\n",
        "    print(f\"\\n{name} classification report:\")\n",
        "    print(classification_report(y_val, yhat, target_names=label_encoder.classes_, digits=3))\n",
        "    acc_lo, acc_med, acc_hi = bootstrap_ci(y_val, yhat, accuracy_score, B=1000)\n",
        "    f1_lo,  f1_med,  f1_hi  = bootstrap_ci(y_val, yhat, lambda a,b: f1_score(a,b,average='weighted'), B=1000)\n",
        "    print_ci(\"accuracy\", acc_lo, acc_med, acc_hi)\n",
        "    print_ci(\"weighted f1\", f1_lo, f1_med, f1_hi)"
      ],
      "metadata": {
        "id": "GZNveaWJoLnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "166l9EC-7Ind"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=num_classes\n",
        ").to(device)\n",
        "\n",
        "# dataset for BERT using existing train/val split\n",
        "class BertTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = list(texts)  # use existing X_train/X_val\n",
        "        self.labels = list(labels)  # use existing y_train/y_val\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self): return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "        enc = self.tokenizer(text, truncation=True, padding='max_length',\n",
        "                           max_length=self.max_length, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': enc['input_ids'].squeeze(0),\n",
        "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "bert_train_ds = BertTextDataset(X_train, y_train, tokenizer)\n",
        "bert_val_ds = BertTextDataset(X_val, y_val, tokenizer)\n",
        "bert_train_loader = DataLoader(bert_train_ds, batch_size=8, shuffle=True)  # smaller batch for stability\n",
        "bert_val_loader = DataLoader(bert_val_ds, batch_size=16, shuffle=False)\n",
        "\n",
        "# training\n",
        "optimizer = optim.AdamW(bert_model.parameters(), lr=2e-5)\n",
        "bert_model.train()\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    for batch in bert_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "        if batch_count % 5 == 0:\n",
        "            print(f\"  Epoch {epoch+1}, Batch {batch_count}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} avg loss: {total_loss/batch_count:.4f}\")\n",
        "\n",
        "# evaluate BERT on same validation set as classical models\n",
        "bert_model.eval()\n",
        "bert_predictions = []\n",
        "bert_true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in bert_val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        bert_predictions.extend(predictions.cpu().numpy())\n",
        "        bert_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "bert_accuracy = accuracy_score(bert_true_labels, bert_predictions)\n",
        "print(f\"BERT accuracy on validation set: {bert_accuracy:.3f}\")\n",
        "\n",
        "# setup bert attributions for comparison with classical models\n",
        "PAD_IND = tokenizer.pad_token_id\n",
        "bert_token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\n",
        "\n",
        "def bert_forward_func(input_ids, attention_mask):\n",
        "    bert_model.eval()\n",
        "    return bert_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "bert_lig = LayerIntegratedGradients(bert_forward_func, bert_model.bert.embeddings.word_embeddings)"
      ],
      "metadata": {
        "id": "OcHq-Cgo7K4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"TF-IDF vocabulary setup for LogReg and SVM interpretability (BERT uses different tokens)"
      ],
      "metadata": {
        "id": "Rf1EBasFtPoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(f\"Total TF-IDF features: {len(feature_names)}\")"
      ],
      "metadata": {
        "id": "SquvHs5KoUp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 IG method for tf-idf + logistic regression, tf-idf + linear svm"
      ],
      "metadata": {
        "id": "SH68pp-5uetK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert TF-IDF data to PyTorch tensors\n",
        "X_val_dense = torch.FloatTensor(Xva_vec.toarray())\n",
        "y_val_tensor = torch.LongTensor(y_val.values)\n",
        "\n",
        "print(f\"Validation data shape: {X_val_dense.shape}\")\n",
        "\n",
        "# create PyTorch wrapper using existing sklearn weights\n",
        "class SklearnToPyTorch(nn.Module):\n",
        "    def __init__(self, sklearn_model):\n",
        "        super().__init__()\n",
        "        # copy weights from trained sklearn model\n",
        "        self.linear = nn.Linear(sklearn_model.coef_.shape[1], sklearn_model.coef_.shape[0])\n",
        "        self.linear.weight.data = torch.FloatTensor(sklearn_model.coef_)\n",
        "        self.linear.bias.data = torch.FloatTensor(sklearn_model.intercept_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# pytorch_model = SklearnToPyTorch(logreg)\n",
        "# pytorch_model.eval()\n",
        "\n",
        "# ig_text = IntegratedGradients(pytorch_model)\n",
        "\n",
        "pytorch_logreg = SklearnToPyTorch(logreg)\n",
        "pytorch_logreg.eval()\n",
        "pytorch_svm = SklearnToPyTorch(linsvm)\n",
        "pytorch_svm.eval()\n",
        "\n",
        "ig_logreg = IntegratedGradients(pytorch_logreg)\n",
        "ig_svm = IntegratedGradients(pytorch_svm)\n"
      ],
      "metadata": {
        "id": "QTXO0B4Auqz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of feature importance vs IG on logress, SVM, and BERT"
      ],
      "metadata": {
        "id": "ETX_x9YgLNjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import visualization\n",
        "\n",
        "sample_indices = [1, 5, 10, 15]\n",
        "sample_texts = [list(X_val)[i] for i in sample_indices if i < len(X_val)]\n",
        "\n",
        "vis_records_logreg_m1 = []  # LogReg Method 1\n",
        "vis_records_logreg_m2 = []  # LogReg Method 2\n",
        "vis_records_svm_m1 = []     # SVM Method 1\n",
        "vis_records_svm_m2 = []     # SVM Method 2\n",
        "vis_records_bert_m1 = []    # BERT Method 1\n",
        "vis_records_bert_m2 = []    # BERT Method 2\n",
        "\n",
        "# convert TF-IDF attributions back to word-level attributions for visualization\n",
        "def create_tfidf_word_attribution_record(sample_text, tfidf_attributions, feature_names, predicted_class, confidence, model_name):\n",
        "    words = sample_text.split()\n",
        "    word_attributions = []\n",
        "\n",
        "    for word in words:\n",
        "        word_attr = 0.0\n",
        "        word_lower = word.lower().strip('.,!?\";')\n",
        "\n",
        "        for i, feature in enumerate(feature_names):\n",
        "            if word_lower in feature.split():\n",
        "                word_attr += tfidf_attributions[i]\n",
        "\n",
        "        word_attributions.append(word_attr)\n",
        "\n",
        "    # normalize\n",
        "    max_attr = max(abs(attr) for attr in word_attributions) if word_attributions else 1\n",
        "    if max_attr > 0:\n",
        "        word_attributions = [attr/max_attr for attr in word_attributions]\n",
        "\n",
        "    return visualization.VisualizationDataRecord(\n",
        "        word_attributions, confidence, f\"{model_name}: {predicted_class}\",\n",
        "        \"\", \"\", sum(word_attributions), words, 0.0\n",
        "    )\n",
        "\n",
        "def create_bert_word_attribution_record(sample_text, attributions, predicted_class, confidence, method_name):\n",
        "    original_words = sample_text.split()\n",
        "\n",
        "    # for Method 1: attributions are per-dimension of [CLS] representation\n",
        "    # for Method 2: attributions are per-token, need to map back to words\n",
        "    if method_name == \"BERT M1\":\n",
        "        # Method 1: Use magnitude of representation attribution as word importance\n",
        "        # Simple approach: distribute the overall importance equally across words\n",
        "        if len(original_words) > 0:\n",
        "            overall_importance = abs(attributions).mean() if hasattr(attributions, 'mean') else abs(sum(attributions))\n",
        "            word_attributions = [overall_importance] * len(original_words)\n",
        "        else:\n",
        "            word_attributions = [0.0]\n",
        "    else:\n",
        "        # Method 2: map BERT tokens back to original words\n",
        "        enc = tokenizer(sample_text, add_special_tokens=True, padding='max_length',\n",
        "                       truncation=True, max_length=128, return_tensors='pt').to(device)\n",
        "        bert_tokens = tokenizer.convert_ids_to_tokens(enc['input_ids'][0])\n",
        "\n",
        "        word_attributions = []\n",
        "        token_idx = 1  # skip [CLS]\n",
        "\n",
        "        for word in original_words:\n",
        "            word_attr = 0.0\n",
        "            word_piece_count = 0\n",
        "\n",
        "            # skip special tokens\n",
        "            while token_idx < len(bert_tokens) and bert_tokens[token_idx] in ['[SEP]', '[PAD]']:\n",
        "                break\n",
        "\n",
        "            # accumulate attributions for word pieces\n",
        "            while token_idx < len(bert_tokens) and bert_tokens[token_idx] not in ['[SEP]', '[PAD]']:\n",
        "                if token_idx < len(attributions):\n",
        "                    word_attr += attributions[token_idx]\n",
        "                    word_piece_count += 1\n",
        "                token_idx += 1\n",
        "\n",
        "                # check if next token is start of new word (doesn't start with ##)\n",
        "                if token_idx < len(bert_tokens) and not bert_tokens[token_idx].startswith('##'):\n",
        "                    break\n",
        "\n",
        "            # average attribution across word pieces\n",
        "            if word_piece_count > 0:\n",
        "                word_attr /= word_piece_count\n",
        "\n",
        "            word_attributions.append(word_attr)\n",
        "\n",
        "    # normalize\n",
        "    max_attr = max(abs(attr) for attr in word_attributions) if word_attributions else 1\n",
        "    if max_attr > 0:\n",
        "        word_attributions = [attr/max_attr for attr in word_attributions]\n",
        "\n",
        "    return visualization.VisualizationDataRecord(\n",
        "        word_attributions, confidence, f\"{method_name}: {predicted_class}\",\n",
        "        \"\", \"\", sum(word_attributions), original_words, 0.0\n",
        "    )\n",
        "\n",
        "for i, (sample_idx, sample_text) in enumerate(zip(sample_indices[:len(sample_texts)], sample_texts)):\n",
        "    if sample_idx >= len(X_val_dense):\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nSAMPLE {sample_idx}: {sample_text}\")\n",
        "    true_class = label_encoder.classes_[y_val.iloc[sample_idx]]\n",
        "    print(f\"True class: {true_class}\")\n",
        "\n",
        "    ##### LOGREG PREDICTIONS AND ATTRIBUTIONS\n",
        "    with torch.no_grad():\n",
        "        sample_tensor = X_val_dense[sample_idx:sample_idx+1]\n",
        "        logreg_logits = pytorch_logreg(sample_tensor)\n",
        "        logreg_pred_idx = torch.argmax(logreg_logits, dim=1).item()\n",
        "        logreg_confidence = torch.softmax(logreg_logits, dim=1)[0][logreg_pred_idx].item()\n",
        "\n",
        "    logreg_pred_class = label_encoder.classes_[logreg_pred_idx]\n",
        "\n",
        "    # LogReg Method 1: feature importance (wi,c × xi)\n",
        "    logreg_coefs = logreg.coef_[logreg_pred_idx]  # wi,c\n",
        "    sample_tfidf = Xva_vec[sample_idx].toarray()[0]  # xi\n",
        "    logreg_m1_attrs = logreg_coefs * sample_tfidf  # Mi,c = wi,c × xi\n",
        "\n",
        "    # LogReg Method 2: integrated gradients\n",
        "    baseline = torch.zeros_like(sample_tensor)\n",
        "    logreg_m2_attrs = ig_logreg.attribute(sample_tensor, baseline, target=logreg_pred_idx, n_steps=50)\n",
        "    logreg_m2_attrs = logreg_m2_attrs.squeeze().detach().numpy()\n",
        "\n",
        "    ##### SVM PREDICTIONS AND ATTRIBUTIONS\n",
        "    svm_pred_idx = linsvm.predict(Xva_vec[sample_idx])[0]\n",
        "    svm_pred_class = label_encoder.classes_[svm_pred_idx]\n",
        "    svm_confidence = 0.8  # SVM doesn't give probabilities easily\n",
        "\n",
        "    # SVM Method 1: feature importance (wi,c × xi)\n",
        "    svm_coefs = linsvm.coef_[svm_pred_idx]  # wi,c\n",
        "    svm_m1_attrs = svm_coefs * sample_tfidf  # Mi,c = wi,c × xi\n",
        "\n",
        "    # SVM Method 2: integrated gradients (same as LogReg since same TF-IDF space)\n",
        "    svm_m2_attrs = logreg_m2_attrs\n",
        "\n",
        "    ##### BERT PREDICTIONS AND ATTRIBUTIONS\n",
        "    # tokenize for BERT\n",
        "    enc = tokenizer(sample_text, add_special_tokens=True, padding='max_length',\n",
        "                   truncation=True, max_length=128, return_tensors='pt').to(device)\n",
        "\n",
        "    # get BERT prediction and [CLS] representation\n",
        "    with torch.no_grad():\n",
        "        bert_outputs = bert_model(**enc, output_hidden_states=True)\n",
        "        bert_logits = bert_outputs.logits\n",
        "        bert_pred_idx = torch.argmax(bert_logits, dim=1).item()\n",
        "        bert_confidence = torch.softmax(bert_logits, dim=1)[0][bert_pred_idx].item()\n",
        "        cls_representation = bert_outputs.hidden_states[-1][0, 0, :]  # [CLS] token from last layer\n",
        "\n",
        "    bert_pred_class = label_encoder.classes_[bert_pred_idx]\n",
        "\n",
        "    # BERT Method 1: feature importance (wi,c × xi)\n",
        "    # wi,c: classification head weights for predicted class\n",
        "    # xi: [CLS] token representation\n",
        "    bert_classification_weights = bert_model.classifier.weight[bert_pred_idx]  # wi,c\n",
        "    bert_m1_attrs = bert_classification_weights * cls_representation  # Mi,c = wi,c × xi\n",
        "    bert_m1_attrs = bert_m1_attrs.detach().cpu().numpy()\n",
        "\n",
        "    # BERT Method 2: integrated gradients\n",
        "    seq_len = int(enc['input_ids'].size(1))\n",
        "    ref = bert_token_reference.generate_reference(seq_len, device=device).unsqueeze(0)\n",
        "\n",
        "    try:\n",
        "        atts_ig, delta = bert_lig.attribute(\n",
        "            inputs=enc['input_ids'], baselines=ref,\n",
        "            additional_forward_args=(enc['attention_mask'],),\n",
        "            target=bert_pred_idx, n_steps=50, return_convergence_delta=True\n",
        "        )\n",
        "        bert_m2_attrs = atts_ig.sum(dim=2).squeeze(0).detach().cpu().numpy()\n",
        "    except:\n",
        "        bert_m2_attrs = [0.0] * seq_len\n",
        "\n",
        "    # visualization\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # TF-IDF models (LogReg and SVM)\n",
        "    vis_records_logreg_m1.append(create_tfidf_word_attribution_record(\n",
        "        sample_text, logreg_m1_attrs, feature_names, logreg_pred_class, logreg_confidence, \"LogReg M1\"))\n",
        "    vis_records_logreg_m2.append(create_tfidf_word_attribution_record(\n",
        "        sample_text, logreg_m2_attrs, feature_names, logreg_pred_class, logreg_confidence, \"LogReg M2\"))\n",
        "    vis_records_svm_m1.append(create_tfidf_word_attribution_record(\n",
        "        sample_text, svm_m1_attrs, feature_names, svm_pred_class, svm_confidence, \"SVM M1\"))\n",
        "    vis_records_svm_m2.append(create_tfidf_word_attribution_record(\n",
        "        sample_text, svm_m2_attrs, feature_names, svm_pred_class, svm_confidence, \"SVM M2\"))\n",
        "\n",
        "    # BERT\n",
        "    vis_records_bert_m1.append(create_bert_word_attribution_record(\n",
        "        sample_text, bert_m1_attrs, bert_pred_class, bert_confidence, \"BERT M1\"))\n",
        "    vis_records_bert_m2.append(create_bert_word_attribution_record(\n",
        "        sample_text, bert_m2_attrs, bert_pred_class, bert_confidence, \"BERT M2\"))\n",
        "\n",
        "\n",
        "# display\n",
        "for i in range(len(sample_texts)):\n",
        "    if i >= len(vis_records_logreg_m1):\n",
        "        break\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"SENTENCE {i+1}: {sample_texts[i]}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nLogistic regression - Method 1 (feature importance: wi,c * xi):\")\n",
        "    try:\n",
        "        _ = visualization.visualize_text([vis_records_logreg_m1[i]])\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\nLogistic regression - Method 2 (integrated gradients):\")\n",
        "    try:\n",
        "        _ = visualization.visualize_text([vis_records_logreg_m2[i]])\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\nSVM - Method 1 (feature importance: wi,c * xi):\")\n",
        "    try:\n",
        "        _ = visualization.visualize_text([vis_records_svm_m1[i]])\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\nSVM - Method 2 (integrated gradients):\")\n",
        "    try:\n",
        "        _ = visualization.visualize_text([vis_records_svm_m2[i]])\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\nBERT - Method 1 (feature importance: wi,c * xi):\")\n",
        "    try:\n",
        "        _ = visualization.visualize_text([vis_records_bert_m1[i]])\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\nBERT - Method 2 (integrated gradients):\")\n",
        "    try:\n",
        "        _ = visualization.visualize_text([vis_records_bert_m2[i]])\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "OexIqCqqvrx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top attributed features for a sample\n",
        "def analyze_text_sample(sample_idx, model, ig_method, feature_names, n_top=10):\n",
        "    sample = X_val_dense[sample_idx:sample_idx+1]\n",
        "    true_label = y_val_tensor[sample_idx].item()\n",
        "\n",
        "    # prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model(sample)\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "        probs = torch.softmax(logits, dim=1)[0]\n",
        "\n",
        "    # attributions\n",
        "    baseline = torch.zeros_like(sample)\n",
        "    attributions = ig_method.attribute(sample, baseline, target=predicted_class, n_steps=50)\n",
        "    attributions = attributions.squeeze().detach().numpy()\n",
        "\n",
        "    print(f\"\\nSample {sample_idx}:\")\n",
        "    print(f\"  True class: {label_encoder.classes_[true_label]}\")\n",
        "    print(f\"  Predicted: {label_encoder.classes_[predicted_class]}\")\n",
        "    print(f\"  Confidence: {probs[predicted_class]:.3f}\")\n",
        "\n",
        "    # top attributed features\n",
        "    top_indices = np.argsort(np.abs(attributions))[-n_top:][::-1]\n",
        "    print(f\"  Top {n_top} attributed features:\")\n",
        "    for idx in top_indices:\n",
        "        if attributions[idx] != 0:  # only show non-zero attributions\n",
        "            print(f\"    {feature_names[idx]}: {attributions[idx]:.4f}\")\n",
        "\n",
        "    return attributions, predicted_class\n",
        "\n",
        "# sample texts from validation set\n",
        "# logreg\n",
        "print(\"\\nAnalyzing individual text samples with IG:\")\n",
        "print(\"\\nLogReg\")\n",
        "sample_attributions = []\n",
        "for i in [0, 5, 10, 15]:\n",
        "    if i < len(X_val_dense):\n",
        "        attrs, pred = analyze_text_sample(i, pytorch_logreg, ig_logreg, feature_names, n_top=8)\n",
        "        sample_attributions.append(attrs)\n",
        "\n",
        "# svm\n",
        "print(\"\\nSVM\")\n",
        "sample_attributions = []\n",
        "for i in [0, 5, 10, 15]:\n",
        "    if i < len(X_val_dense):\n",
        "        attrs, pred = analyze_text_sample(i, pytorch_svm, ig_svm, feature_names, n_top=8)\n",
        "        sample_attributions.append(attrs)"
      ],
      "metadata": {
        "id": "R66QeusPeo7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7kfWL-DBHim"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}