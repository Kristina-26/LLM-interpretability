{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM+3kduaomev0k1fqYn4gnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kristina-26/LLM-interpretability/blob/main/Pop_ConflictQA_hidden_states_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxQCjXmyU9f6"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers>=4.44.2 accelerate>=0.33.0 bitsandbytes>=0.44.0\n",
        "# !pip install -q scipy matplotlib seaborn scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats as spstats\n",
        "from scipy.special import erf\n",
        "from scipy.stats import bootstrap\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve"
      ],
      "metadata": {
        "id": "DqM3pEKhVVRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "UCnCq5TqVaV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the model you want to run:\n",
        "# Options:\n",
        "#   \"meta-llama/Llama-3.1-8B\"\n",
        "#   \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "#   \"Qwen/Qwen3-14B\"\n",
        "#   \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "#   \"google/gemma-3-12b-it\"\n",
        "#   \"google/gemma-3-4b-it\"\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B\""
      ],
      "metadata": {
        "id": "dp8DIxHWjgyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "CONFIG = {\n",
        "    'model': {\n",
        "        'name': MODEL_NAME,\n",
        "        'use_4bit': False,\n",
        "        'max_length': 256,\n",
        "        'dtype': 'bfloat16',\n",
        "    },\n",
        "    'data': {\n",
        "        'n_items_total': 6948, # 6948 in total\n",
        "        'n_baseline_items': 600,  # From TRAIN only\n",
        "        'test_size': 0.2,\n",
        "        'random_state': 42,\n",
        "    },\n",
        "    'processing': {\n",
        "        'batch_size': 8,\n",
        "        'layer_slice_start': 1,\n",
        "    },\n",
        "    'analysis': {\n",
        "        'last_layers_band': 4,\n",
        "        'top_k_activations': 20,\n",
        "        'threshold_percentile': 95,\n",
        "    },\n",
        "    'contrastive': {\n",
        "        'enabled': True,\n",
        "        'epochs': 40,\n",
        "        'batch_size': 256,\n",
        "        'learning_rate': 0.001,\n",
        "        'weight_decay': 0.0001,\n",
        "        'temperature': 0.07,\n",
        "        'projection_dim': 64,\n",
        "        'hidden_dim': 256,\n",
        "        'last_k_layers': 16,\n",
        "    },\n",
        "    'bootstrap': {\n",
        "        'n_resamples': 1000,\n",
        "        'confidence_level': 0.95,\n",
        "    },\n",
        "    'seed': 42,\n",
        "}\n",
        "\n",
        "# Set seeds\n",
        "random.seed(CONFIG['seed'])\n",
        "np.random.seed(CONFIG['seed'])\n",
        "torch.manual_seed(CONFIG['seed'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(CONFIG['seed'])\n"
      ],
      "metadata": {
        "id": "O6T-CcNKVcbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "def normalize_text(s: str) -> str:\n",
        "    \"\"\"Normalize text for comparison.\"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[^a-z0-9]+', ' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return s.strip()\n",
        "\n",
        "def postprocess_short_phrase(text: str, max_words: int = 6) -> str:\n",
        "    \"\"\"Extract short answer from model output.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    t = text.strip()\n",
        "    t = re.split(r'[\\n\\r]|[.?!]', t, maxsplit=1)[0]\n",
        "    patterns = [r'^(the answer is|it is|it\\'s|this is|answer:)\\s+']\n",
        "    for pattern in patterns:\n",
        "        t = re.sub(pattern, '', t, flags=re.I)\n",
        "    t = t.strip(\" \\\"'\"\"'':,;-\")\n",
        "    tokens = t.split()\n",
        "    return \" \".join(tokens[:max_words]) if tokens else \"\"\n",
        "\n",
        "def gt_overlap(answer: str, gt_list: List[str]) -> bool:\n",
        "    \"\"\"Check if answer overlaps with ground truth.\"\"\"\n",
        "    answer_words = {w for w in normalize_text(answer).split() if len(w) >= 3}\n",
        "    if not answer_words:\n",
        "        return False\n",
        "    for gt in gt_list:\n",
        "        gt_words = {w for w in normalize_text(gt).split() if len(w) >= 3}\n",
        "        if gt_words and (gt_words <= answer_words or len(answer_words & gt_words) > 0):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def short_context_sentence(s: str, max_chars: int = 200) -> str:\n",
        "    \"\"\"Extract first sentence from context.\"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    parts = re.split(r'(?<!\\b[A-Z]\\.)(?<=[.!?])\\s+', s.strip(), maxsplit=1)\n",
        "    first = parts[0]\n",
        "    if len(first) > max_chars:\n",
        "        first = first[:max_chars].rsplit(' ', 1)[0]\n",
        "    return first\n",
        "\n",
        "def classify_domain(question: str) -> str:\n",
        "    \"\"\"Classify question domain.\"\"\"\n",
        "    q_lower = question.lower()\n",
        "    domain_keywords = {\n",
        "        \"geography\": [\"capital\", \"country\", \"city\", \"continent\", \"ocean\"],\n",
        "        \"history\": [\"year\", \"century\", \"war\", \"president\", \"king\", \"queen\"],\n",
        "        \"science\": [\"chemical\", \"planet\", \"element\", \"species\", \"theory\"],\n",
        "        \"art\": [\"wrote\", \"author\", \"book\", \"novel\", \"poem\", \"play\"],\n",
        "        \"math\": [\"plus\", \"+\", \"minus\", \"-\", \"times\", \"×\", \"divided\"],\n",
        "    }\n",
        "    for domain, keywords in domain_keywords.items():\n",
        "        if any(kw in q_lower for kw in keywords):\n",
        "            return domain\n",
        "    return \"general\"\n",
        "\n",
        "def compute_confidence_interval(data: np.ndarray,\n",
        "                               statistic_fn,\n",
        "                               confidence_level: float = 0.95,\n",
        "                               n_resamples: int = 1000,\n",
        "                               method: str = 'percentile') -> Tuple[float, float, float]:\n",
        "    \"\"\"Compute statistic with confidence interval using bootstrap.\"\"\"\n",
        "    if len(data) == 0:\n",
        "        return np.nan, np.nan, np.nan\n",
        "\n",
        "    point_estimate = statistic_fn(data)\n",
        "\n",
        "    try:\n",
        "        rng = np.random.default_rng(CONFIG['seed'])\n",
        "        result = bootstrap(\n",
        "            (data,),\n",
        "            lambda x: statistic_fn(x[0]),\n",
        "            n_resamples=n_resamples,\n",
        "            confidence_level=confidence_level,\n",
        "            random_state=rng,\n",
        "            method=method\n",
        "        )\n",
        "        return point_estimate, result.confidence_interval.low, result.confidence_interval.high\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Bootstrap failed: {e}\")\n",
        "        return point_estimate, np.nan, np.nan\n",
        "\n",
        "\n",
        "def bootstrap_auroc_ci(\n",
        "    y_true: np.ndarray,\n",
        "    scores: np.ndarray,\n",
        "    n_resamples: int = 1000,\n",
        "    confidence_level: float = 0.95,\n",
        "    seed: int = 42,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Compute bootstrap CI for AUROC, skipping resamples with only one class.\n",
        "    Returns:(ci_low, ci_high)\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    scores = np.asarray(scores)\n",
        "    n = len(y_true)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    aurocs = []\n",
        "\n",
        "    for _ in range(n_resamples):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        y_boot = y_true[idx]\n",
        "        s_boot = scores[idx]\n",
        "\n",
        "        # skip degenerate resamples with only one class\n",
        "        if len(np.unique(y_boot)) < 2:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            aurocs.append(roc_auc_score(y_boot, s_boot))\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if len(aurocs) == 0:\n",
        "        # no valid resamples, cannot form a CI\n",
        "        raise RuntimeError(\"All bootstrap resamples had only one class, cannot compute AUROC CI.\")\n",
        "\n",
        "    aurocs = np.array(aurocs, dtype=float)\n",
        "    alpha = 1.0 - confidence_level\n",
        "    ci_low = np.percentile(aurocs, 100 * alpha / 2.0)\n",
        "    ci_high = np.percentile(aurocs, 100 * (1.0 - alpha / 2.0))\n",
        "\n",
        "    logger.info(\n",
        "        f\"Bootstrap AUROC CI: used {len(aurocs)} valid resamples out of {n_resamples} \"\n",
        "        f\"(skipped {n_resamples - len(aurocs)})\"\n",
        "    )\n",
        "\n",
        "    return ci_low, ci_high\n",
        "\n",
        "\n",
        "def analyze_misclassifications(test_items, y_test, predictions, scores):\n",
        "    # indices of false positives and false negatives\n",
        "    fp_idx = np.where((y_test == 0) & (predictions == 1))[0]\n",
        "    fn_idx = np.where((y_test == 1) & (predictions == 0))[0]\n",
        "\n",
        "    fp_domains = Counter([test_items[i].domain for i in fp_idx])\n",
        "    fn_domains = Counter([test_items[i].domain for i in fn_idx])\n",
        "\n",
        "    fp_scores = scores[fp_idx] if len(fp_idx) > 0 else np.array([])\n",
        "    fn_scores = scores[fn_idx] if len(fn_idx) > 0 else np.array([])\n",
        "\n",
        "    results = {\n",
        "        \"false_positives\": {\n",
        "            \"count\": int(len(fp_idx)),\n",
        "            \"domains\": dict(fp_domains),\n",
        "            \"score_mean\": float(fp_scores.mean()) if fp_scores.size > 0 else None,\n",
        "            \"score_std\": float(fp_scores.std()) if fp_scores.size > 0 else None,\n",
        "            \"example_indices\": fp_idx[:5].tolist(),\n",
        "        },\n",
        "        \"false_negatives\": {\n",
        "            \"count\": int(len(fn_idx)),\n",
        "            \"domains\": dict(fn_domains),\n",
        "            \"score_mean\": float(fn_scores.mean()) if fn_scores.size > 0 else None,\n",
        "            \"score_std\": float(fn_scores.std()) if fn_scores.size > 0 else None,\n",
        "            \"example_indices\": fn_idx[:5].tolist(),\n",
        "        },\n",
        "    }\n",
        "    return results\n",
        "\n",
        "def summarize_layer_importance(auroc_by_layer: Dict[str, np.ndarray]) -> List[str]:\n",
        "    summaries = []\n",
        "    for feat_name, aurocs in auroc_by_layer.items():\n",
        "        if np.all(np.isnan(aurocs)):\n",
        "            continue\n",
        "        best_layer = int(np.nanargmax(aurocs))\n",
        "        best_score = float(aurocs[best_layer])\n",
        "\n",
        "        L = len(aurocs)\n",
        "        if best_layer < 0.3 * L:\n",
        "            region = \"early layers\"\n",
        "        elif best_layer < 0.7 * L:\n",
        "            region = \"middle layers\"\n",
        "        else:\n",
        "            region = \"late layers\"\n",
        "\n",
        "        summaries.append(\n",
        "            f\"{feat_name}: strongest signal in {region} \"\n",
        "            f\"(layer {best_layer+1}, AUROC={best_score:.3f})\"\n",
        "        )\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "7lAb3xjtVo0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_results_and_items(results_path: str = \"results.json\"):\n",
        "    \"\"\"Load the saved results and reconstruct test items if possible.\"\"\"\n",
        "    with open(results_path, 'r') as f:\n",
        "        results = json.load(f)\n",
        "    return results\n",
        "\n",
        "def detailed_error_analysis(\n",
        "    test_items: List,\n",
        "    test_results: List,\n",
        "    y_test: np.ndarray,\n",
        "    predictions: np.ndarray,\n",
        "    scores: np.ndarray,\n",
        "    method_name: str = \"Method\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Perform detailed error analysis on misclassifications.\n",
        "\n",
        "    Args:\n",
        "        test_items: List of Item objects\n",
        "        test_results: List of EvaluationResult objects\n",
        "        y_test: True labels (1=flip, 0=non-flip)\n",
        "        predictions: Binary predictions\n",
        "        scores: Continuous scores from the model\n",
        "        method_name: Name of the detection method\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with detailed analysis\n",
        "    \"\"\"\n",
        "\n",
        "    # Identify errors\n",
        "    fp_idx = np.where((y_test == 0) & (predictions == 1))[0]\n",
        "    fn_idx = np.where((y_test == 1) & (predictions == 0))[0]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"DETAILED ERROR ANALYSIS: {method_name}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total test samples: {len(y_test)}\")\n",
        "    print(f\"  True flips (y=1): {y_test.sum()}\")\n",
        "    print(f\"  True non-flips (y=0): {(y_test == 0).sum()}\")\n",
        "    print(f\"False Positives: {len(fp_idx)} (predicted flip, actually non-flip)\")\n",
        "    print(f\"False Negatives: {len(fn_idx)} (predicted non-flip, actually flip)\")\n",
        "\n",
        "    analysis = {\n",
        "        \"method\": method_name,\n",
        "        \"total_samples\": int(len(y_test)),\n",
        "        \"n_flips\": int(y_test.sum()),\n",
        "        \"n_non_flips\": int((y_test == 0).sum()),\n",
        "        \"false_positives\": {\n",
        "            \"count\": int(len(fp_idx)),\n",
        "            \"rate\": float(len(fp_idx) / (y_test == 0).sum()) if (y_test == 0).sum() > 0 else 0,\n",
        "            \"examples\": [],\n",
        "            \"patterns\": {}\n",
        "        },\n",
        "        \"false_negatives\": {\n",
        "            \"count\": int(len(fn_idx)),\n",
        "            \"rate\": float(len(fn_idx) / y_test.sum()) if y_test.sum() > 0 else 0,\n",
        "            \"examples\": [],\n",
        "            \"patterns\": {}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # =================================================================\n",
        "    # FALSE POSITIVES (model says flip, but it's actually non-flip)\n",
        "    # =================================================================\n",
        "    if len(fp_idx) > 0:\n",
        "        print(\"\\n\" + \"-\"*70)\n",
        "        print(\"FALSE POSITIVES (Model incorrectly predicted flip)\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Score statistics\n",
        "        fp_scores = scores[fp_idx]\n",
        "        print(f\"\\nScore statistics:\")\n",
        "        print(f\"  Mean: {fp_scores.mean():.4f}\")\n",
        "        print(f\"  Std:  {fp_scores.std():.4f}\")\n",
        "        print(f\"  Min:  {fp_scores.min():.4f}\")\n",
        "        print(f\"  Max:  {fp_scores.max():.4f}\")\n",
        "\n",
        "        # Domain distribution\n",
        "        fp_domains = Counter([test_items[i].domain for i in fp_idx])\n",
        "        print(f\"\\nDomain distribution:\")\n",
        "        for domain, count in fp_domains.most_common():\n",
        "            pct = 100 * count / len(fp_idx)\n",
        "            print(f\"  {domain:15s}: {count:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "        analysis[\"false_positives\"][\"patterns\"][\"domains\"] = dict(fp_domains)\n",
        "        analysis[\"false_positives\"][\"patterns\"][\"score_mean\"] = float(fp_scores.mean())\n",
        "        analysis[\"false_positives\"][\"patterns\"][\"score_std\"] = float(fp_scores.std())\n",
        "\n",
        "        # Analyze why model was misled (look at answer patterns)\n",
        "        # Check if incorrect context actually made model change answer\n",
        "        answer_changed = []\n",
        "        for idx in fp_idx:\n",
        "            res = test_results[idx]\n",
        "            # Non-flip means: base was correct AND incorrect was also correct\n",
        "            # OR base was wrong AND incorrect was also wrong\n",
        "            changed = (res.answer_base != res.answer_incorrect)\n",
        "            answer_changed.append(changed)\n",
        "\n",
        "        n_changed = sum(answer_changed)\n",
        "        print(f\"\\nAnswer behavior:\")\n",
        "        print(f\"  Incorrect context changed answer: {n_changed}/{len(fp_idx)} \"\n",
        "              f\"({100*n_changed/len(fp_idx):.1f}%)\")\n",
        "        analysis[\"false_positives\"][\"patterns\"][\"incorrect_changed_answer\"] = int(n_changed)\n",
        "\n",
        "        # Look at question length\n",
        "        question_lengths = [len(test_items[i].question.split()) for i in fp_idx]\n",
        "        print(f\"\\nQuestion length:\")\n",
        "        print(f\"  Mean: {np.mean(question_lengths):.1f} words\")\n",
        "        print(f\"  Median: {np.median(question_lengths):.1f} words\")\n",
        "        analysis[\"false_positives\"][\"patterns\"][\"question_length_mean\"] = float(np.mean(question_lengths))\n",
        "\n",
        "        # Look at context length\n",
        "        context_lengths = [len(test_items[i].incorrect_ctx.split()) for i in fp_idx]\n",
        "        print(f\"\\nIncorrect context length:\")\n",
        "        print(f\"  Mean: {np.mean(context_lengths):.1f} words\")\n",
        "        print(f\"  Median: {np.median(context_lengths):.1f} words\")\n",
        "        analysis[\"false_positives\"][\"patterns\"][\"context_length_mean\"] = float(np.mean(context_lengths))\n",
        "\n",
        "        # Show top 10 examples with highest scores (most confident FPs)\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"Top 10 FALSE POSITIVES (most confident errors):\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        sorted_fp_idx = fp_idx[np.argsort(fp_scores)[::-1]][:10]\n",
        "\n",
        "        for rank, idx in enumerate(sorted_fp_idx, 1):\n",
        "            item = test_items[idx]\n",
        "            res = test_results[idx]\n",
        "            score = scores[idx]\n",
        "\n",
        "            example = {\n",
        "                \"rank\": rank,\n",
        "                \"score\": float(score),\n",
        "                \"question\": item.question,\n",
        "                \"domain\": item.domain,\n",
        "                \"ground_truth\": item.gt_list,\n",
        "                \"answer_base\": res.answer_base,\n",
        "                \"answer_correct\": res.answer_correct,\n",
        "                \"answer_incorrect\": res.answer_incorrect,\n",
        "                \"correct_ctx\": item.correct_ctx,\n",
        "                \"incorrect_ctx\": item.incorrect_ctx,\n",
        "                \"base_correct\": res.base_correct,\n",
        "                \"correct_correct\": res.correct_correct,\n",
        "                \"incorrect_correct\": res.incorrect_correct,\n",
        "            }\n",
        "\n",
        "            print(f\"\\n#{rank} | Score: {score:.4f} | Domain: {item.domain}\")\n",
        "            print(f\"  Question: {item.question}\")\n",
        "            print(f\"  Ground truth: {item.gt_list}\")\n",
        "            print(f\"  Base answer: '{res.answer_base}' (correct: {res.base_correct})\")\n",
        "            print(f\"  Correct ctx answer: '{res.answer_correct}' (correct: {res.correct_correct})\")\n",
        "            print(f\"  Incorrect ctx answer: '{res.answer_incorrect}' (correct: {res.incorrect_correct})\")\n",
        "            print(f\"  Correct context: {item.correct_ctx[:100]}...\")\n",
        "            print(f\"  Incorrect context: {item.incorrect_ctx[:100]}...\")\n",
        "            print(f\"  → Model thought this was a flip, but both contexts gave {'correct' if res.incorrect_correct else 'wrong'} answers\")\n",
        "\n",
        "            if rank <= 5:  # Save first 5 examples\n",
        "                analysis[\"false_positives\"][\"examples\"].append(example)\n",
        "\n",
        "    # =================================================================\n",
        "    # FALSE NEGATIVES (model says non-flip, but it's actually a flip)\n",
        "    # =================================================================\n",
        "    if len(fn_idx) > 0:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FALSE NEGATIVES (Model failed to detect flip)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Score statistics\n",
        "        fn_scores = scores[fn_idx]\n",
        "        print(f\"\\nScore statistics:\")\n",
        "        print(f\"  Mean: {fn_scores.mean():.4f}\")\n",
        "        print(f\"  Std:  {fn_scores.std():.4f}\")\n",
        "        print(f\"  Min:  {fn_scores.min():.4f}\")\n",
        "        print(f\"  Max:  {fn_scores.max():.4f}\")\n",
        "\n",
        "        # Domain distribution\n",
        "        fn_domains = Counter([test_items[i].domain for i in fn_idx])\n",
        "        print(f\"\\nDomain distribution:\")\n",
        "        for domain, count in fn_domains.most_common():\n",
        "            pct = 100 * count / len(fn_idx)\n",
        "            print(f\"  {domain:15s}: {count:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "        analysis[\"false_negatives\"][\"patterns\"][\"domains\"] = dict(fn_domains)\n",
        "        analysis[\"false_negatives\"][\"patterns\"][\"score_mean\"] = float(fn_scores.mean())\n",
        "        analysis[\"false_negatives\"][\"patterns\"][\"score_std\"] = float(fn_scores.std())\n",
        "\n",
        "        # Check base model performance\n",
        "        base_was_correct = [test_results[i].base_correct for i in fn_idx]\n",
        "        n_base_correct = sum(base_was_correct)\n",
        "        print(f\"\\nBase model (no context) performance:\")\n",
        "        print(f\"  Base was correct: {n_base_correct}/{len(fn_idx)} \"\n",
        "              f\"({100*n_base_correct/len(fn_idx):.1f}%)\")\n",
        "        analysis[\"false_negatives\"][\"patterns\"][\"base_correct\"] = int(n_base_correct)\n",
        "\n",
        "        # Question and context lengths\n",
        "        question_lengths = [len(test_items[i].question.split()) for i in fn_idx]\n",
        "        context_lengths = [len(test_items[i].incorrect_ctx.split()) for i in fn_idx]\n",
        "\n",
        "        print(f\"\\nQuestion length:\")\n",
        "        print(f\"  Mean: {np.mean(question_lengths):.1f} words\")\n",
        "        print(f\"  Median: {np.median(question_lengths):.1f} words\")\n",
        "\n",
        "        print(f\"\\nIncorrect context length:\")\n",
        "        print(f\"  Mean: {np.mean(context_lengths):.1f} words\")\n",
        "        print(f\"  Median: {np.median(context_lengths):.1f} words\")\n",
        "\n",
        "        analysis[\"false_negatives\"][\"patterns\"][\"question_length_mean\"] = float(np.mean(question_lengths))\n",
        "        analysis[\"false_negatives\"][\"patterns\"][\"context_length_mean\"] = float(np.mean(context_lengths))\n",
        "\n",
        "        # Analyze how similar incorrect answer is to correct answer\n",
        "        # (maybe model gave \"close\" answer that still triggered overlap)\n",
        "        close_answers = 0\n",
        "        for idx in fn_idx:\n",
        "            res = test_results[idx]\n",
        "            # Both base and incorrect gave answers - check if they're similar\n",
        "            if res.answer_base and res.answer_incorrect:\n",
        "                base_words = set(res.answer_base.lower().split())\n",
        "                inc_words = set(res.answer_incorrect.lower().split())\n",
        "                if len(base_words & inc_words) > 0:\n",
        "                    close_answers += 1\n",
        "\n",
        "        print(f\"\\nAnswer similarity:\")\n",
        "        print(f\"  Base and incorrect answers share words: {close_answers}/{len(fn_idx)} \"\n",
        "              f\"({100*close_answers/len(fn_idx):.1f}%)\")\n",
        "        analysis[\"false_negatives\"][\"patterns\"][\"similar_answers\"] = int(close_answers)\n",
        "\n",
        "        # Show bottom 10 examples with lowest scores (most confident FNs)\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"Top 10 FALSE NEGATIVES (missed flips with lowest scores):\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        sorted_fn_idx = fn_idx[np.argsort(fn_scores)][:10]\n",
        "\n",
        "        for rank, idx in enumerate(sorted_fn_idx, 1):\n",
        "            item = test_items[idx]\n",
        "            res = test_results[idx]\n",
        "            score = scores[idx]\n",
        "\n",
        "            example = {\n",
        "                \"rank\": rank,\n",
        "                \"score\": float(score),\n",
        "                \"question\": item.question,\n",
        "                \"domain\": item.domain,\n",
        "                \"ground_truth\": item.gt_list,\n",
        "                \"answer_base\": res.answer_base,\n",
        "                \"answer_correct\": res.answer_correct,\n",
        "                \"answer_incorrect\": res.answer_incorrect,\n",
        "                \"correct_ctx\": item.correct_ctx,\n",
        "                \"incorrect_ctx\": item.incorrect_ctx,\n",
        "                \"base_correct\": res.base_correct,\n",
        "                \"correct_correct\": res.correct_correct,\n",
        "                \"incorrect_correct\": res.incorrect_correct,\n",
        "            }\n",
        "\n",
        "            print(f\"\\n#{rank} | Score: {score:.4f} | Domain: {item.domain}\")\n",
        "            print(f\"  Question: {item.question}\")\n",
        "            print(f\"  Ground truth: {item.gt_list}\")\n",
        "            print(f\"  Base answer: '{res.answer_base}' (correct: {res.base_correct})\")\n",
        "            print(f\"  Correct ctx answer: '{res.answer_correct}' (correct: {res.correct_correct})\")\n",
        "            print(f\"  Incorrect ctx answer: '{res.answer_incorrect}' (correct: {res.incorrect_correct})\")\n",
        "            print(f\"  Correct context: {item.correct_ctx[:100]}...\")\n",
        "            print(f\"  Incorrect context: {item.incorrect_ctx[:100]}...\")\n",
        "            print(f\"  → This WAS a flip (base correct, incorrect wrong), but model missed it\")\n",
        "\n",
        "            if rank <= 5:  # Save first 5 examples\n",
        "                analysis[\"false_negatives\"][\"examples\"].append(example)\n",
        "\n",
        "    # =================================================================\n",
        "    # COMPARATIVE PATTERNS\n",
        "    # =================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPARATIVE PATTERNS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if len(fp_idx) > 0 and len(fn_idx) > 0:\n",
        "        fp_domains = Counter([test_items[i].domain for i in fp_idx])\n",
        "        fn_domains = Counter([test_items[i].domain for i in fn_idx])\n",
        "\n",
        "        all_domains = set(fp_domains.keys()) | set(fn_domains.keys())\n",
        "\n",
        "        print(\"\\nError rates by domain:\")\n",
        "        print(f\"{'Domain':<15} {'FP Rate':<10} {'FN Rate':<10} {'Harder for':<15}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for domain in sorted(all_domains):\n",
        "            # Total samples in this domain\n",
        "            domain_indices = [i for i in range(len(test_items)) if test_items[i].domain == domain]\n",
        "            n_domain = len(domain_indices)\n",
        "\n",
        "            if n_domain == 0:\n",
        "                continue\n",
        "\n",
        "            fp_count = fp_domains.get(domain, 0)\n",
        "            fn_count = fn_domains.get(domain, 0)\n",
        "\n",
        "            fp_rate = fp_count / n_domain\n",
        "            fn_rate = fn_count / n_domain\n",
        "\n",
        "            harder_for = \"FP\" if fp_rate > fn_rate else \"FN\" if fn_rate > fp_rate else \"Equal\"\n",
        "\n",
        "            print(f\"{domain:<15} {fp_rate:<10.3f} {fn_rate:<10.3f} {harder_for:<15}\")\n",
        "\n",
        "        # Question length comparison\n",
        "        fp_q_len = np.mean([len(test_items[i].question.split()) for i in fp_idx])\n",
        "        fn_q_len = np.mean([len(test_items[i].question.split()) for i in fn_idx])\n",
        "\n",
        "        print(f\"\\nAverage question length:\")\n",
        "        print(f\"  False Positives: {fp_q_len:.1f} words\")\n",
        "        print(f\"  False Negatives: {fn_q_len:.1f} words\")\n",
        "        print(f\"  Difference: {abs(fp_q_len - fn_q_len):.1f} words \"\n",
        "              f\"({'FP longer' if fp_q_len > fn_q_len else 'FN longer'})\")\n",
        "\n",
        "    return analysis\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# USAGE EXAMPLE\n",
        "# =================================================================\n",
        "\n",
        "def run_error_analysis_on_saved_data(\n",
        "    test_items,\n",
        "    test_results,\n",
        "    y_test,\n",
        "    test_scores_baseline,\n",
        "    test_predictions_baseline,\n",
        "    test_scores_contrastive=None,\n",
        "    test_predictions_contrastive=None,\n",
        "    output_dir=Path(\"results\")\n",
        "):\n",
        "    \"\"\"\n",
        "    Run error analysis on saved test data.\n",
        "\n",
        "    Args:\n",
        "        test_items: List of Item objects from test set\n",
        "        test_results: List of EvaluationResult objects from test set\n",
        "        y_test: True labels\n",
        "        test_scores_baseline: Scores from baseline method\n",
        "        test_predictions_baseline: Predictions from baseline method\n",
        "        test_scores_contrastive: Optional scores from contrastive method\n",
        "        test_predictions_contrastive: Optional predictions from contrastive method\n",
        "        output_dir: Directory to save results\n",
        "    \"\"\"\n",
        "\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    all_analyses = {}\n",
        "\n",
        "    # Analyze baseline method\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ANALYZING BASELINE METHOD (ΔΔ MAD)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    baseline_analysis = detailed_error_analysis(\n",
        "        test_items,\n",
        "        test_results,\n",
        "        y_test,\n",
        "        test_predictions_baseline,\n",
        "        test_scores_baseline,\n",
        "        method_name=\"Baseline (ΔΔ MAD)\"\n",
        "    )\n",
        "    all_analyses[\"baseline\"] = baseline_analysis\n",
        "\n",
        "    # Analyze contrastive method if available\n",
        "    if test_scores_contrastive is not None and test_predictions_contrastive is not None:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"ANALYZING CONTRASTIVE LEARNING METHOD\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        contrastive_analysis = detailed_error_analysis(\n",
        "            test_items,\n",
        "            test_results,\n",
        "            y_test,\n",
        "            test_predictions_contrastive,\n",
        "            test_scores_contrastive,\n",
        "            method_name=\"Contrastive Learning\"\n",
        "        )\n",
        "        all_analyses[\"contrastive\"] = contrastive_analysis\n",
        "\n",
        "        # Compare the two methods\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"COMPARISON: BASELINE vs CONTRASTIVE\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        baseline_fp = set(np.where((y_test == 0) & (test_predictions_baseline == 1))[0])\n",
        "        baseline_fn = set(np.where((y_test == 1) & (test_predictions_baseline == 0))[0])\n",
        "\n",
        "        contrastive_fp = set(np.where((y_test == 0) & (test_predictions_contrastive == 1))[0])\n",
        "        contrastive_fn = set(np.where((y_test == 1) & (test_predictions_contrastive == 0))[0])\n",
        "\n",
        "        # Errors unique to each method\n",
        "        baseline_only_fp = baseline_fp - contrastive_fp\n",
        "        contrastive_only_fp = contrastive_fp - baseline_fp\n",
        "        baseline_only_fn = baseline_fn - contrastive_fn\n",
        "        contrastive_only_fn = contrastive_fn - baseline_fn\n",
        "\n",
        "        print(f\"\\nFalse Positives:\")\n",
        "        print(f\"  Baseline only: {len(baseline_only_fp)}\")\n",
        "        print(f\"  Contrastive only: {len(contrastive_only_fp)}\")\n",
        "        print(f\"  Both methods: {len(baseline_fp & contrastive_fp)}\")\n",
        "\n",
        "        print(f\"\\nFalse Negatives:\")\n",
        "        print(f\"  Baseline only: {len(baseline_only_fn)}\")\n",
        "        print(f\"  Contrastive only: {len(contrastive_only_fn)}\")\n",
        "        print(f\"  Both methods: {len(baseline_fn & contrastive_fn)}\")\n",
        "\n",
        "        # What did contrastive fix?\n",
        "        print(f\"\\nContrastive improvements:\")\n",
        "        print(f\"  Fixed FPs: {len(baseline_only_fp)}\")\n",
        "        print(f\"  Fixed FNs: {len(baseline_only_fn)}\")\n",
        "        print(f\"  New FPs: {len(contrastive_only_fp)}\")\n",
        "        print(f\"  New FNs: {len(contrastive_only_fn)}\")\n",
        "\n",
        "        all_analyses[\"comparison\"] = {\n",
        "            \"baseline_only_fp\": int(len(baseline_only_fp)),\n",
        "            \"contrastive_only_fp\": int(len(contrastive_only_fp)),\n",
        "            \"baseline_only_fn\": int(len(baseline_only_fn)),\n",
        "            \"contrastive_only_fn\": int(len(contrastive_only_fn)),\n",
        "            \"fixed_fp\": int(len(baseline_only_fp)),\n",
        "            \"fixed_fn\": int(len(baseline_only_fn)),\n",
        "        }\n",
        "\n",
        "    # Save all analyses\n",
        "    output_file = output_dir / \"error_analysis.json\"\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(all_analyses, f, indent=2)\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(f\"Error analysis saved to: {output_file}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return all_analyses"
      ],
      "metadata": {
        "id": "YRk8EDP4eX5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data structures\n",
        "@dataclass\n",
        "class Item:\n",
        "    \"\"\"Represents a single ConflictQA item.\"\"\"\n",
        "    question: str\n",
        "    gt_list: List[str]\n",
        "    correct_ctx: str\n",
        "    incorrect_ctx: str\n",
        "    domain: str = \"general\"\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "@dataclass\n",
        "class EvaluationResult:\n",
        "    \"\"\"Stores evaluation results for an item.\"\"\"\n",
        "    item_idx: int\n",
        "    answer_base: str\n",
        "    answer_correct: str\n",
        "    answer_incorrect: str\n",
        "    base_correct: bool\n",
        "    correct_correct: bool\n",
        "    incorrect_correct: bool\n",
        "    is_flip: bool  # base correct AND incorrect wrong\n",
        "\n",
        "\n",
        "def build_items_from_raw(raw_data: List[Dict[str, Any]]) -> List[Item]:\n",
        "    \"\"\"Build Item objects from raw JSON data.\"\"\"\n",
        "    items = []\n",
        "    skip_reasons = Counter()\n",
        "\n",
        "    for idx, ex in enumerate(raw_data):\n",
        "        try:\n",
        "            q = str(ex.get(\"question\", \"\")).strip()\n",
        "            if not q:\n",
        "                skip_reasons[\"missing_question\"] += 1\n",
        "                continue\n",
        "\n",
        "            gt = ex.get(\"ground_truth\", [])\n",
        "            if isinstance(gt, str):\n",
        "                gt_list = [gt] if gt.strip() else []\n",
        "            elif isinstance(gt, list):\n",
        "                gt_list = [str(x).strip() for x in gt if str(x).strip()]\n",
        "            else:\n",
        "                gt_list = []\n",
        "\n",
        "            if not gt_list:\n",
        "                skip_reasons[\"missing_ground_truth\"] += 1\n",
        "                continue\n",
        "\n",
        "            mem = str(ex.get(\"memory_answer\", ex.get(\"memory\", \"\"))).strip()\n",
        "            ctr = str(ex.get(\"counter_answer\", ex.get(\"counter\", \"\"))).strip()\n",
        "\n",
        "            if not mem and not ctr:\n",
        "                skip_reasons[\"missing_contexts\"] += 1\n",
        "                continue\n",
        "\n",
        "            mem_aligns = gt_overlap(mem, gt_list) if mem else False\n",
        "            ctr_aligns = gt_overlap(ctr, gt_list) if ctr else False\n",
        "\n",
        "            if mem_aligns and not ctr_aligns:\n",
        "                correct_ctx = short_context_sentence(mem)\n",
        "                incorrect_ctx = short_context_sentence(ctr)\n",
        "            elif ctr_aligns and not mem_aligns:\n",
        "                correct_ctx = short_context_sentence(ctr)\n",
        "                incorrect_ctx = short_context_sentence(mem)\n",
        "            else:\n",
        "                skip_reasons[\"ambiguous_alignment\"] += 1\n",
        "                continue\n",
        "\n",
        "            items.append(Item(\n",
        "                question=q,\n",
        "                gt_list=gt_list,\n",
        "                correct_ctx=correct_ctx,\n",
        "                incorrect_ctx=incorrect_ctx,\n",
        "                domain=classify_domain(q),\n",
        "                metadata={\"source_index\": idx}\n",
        "            ))\n",
        "\n",
        "        except Exception as e:\n",
        "            skip_reasons[f\"error_{type(e).__name__}\"] += 1\n",
        "            if len(skip_reasons) <= 3:\n",
        "                logger.warning(f\"Skipping item {idx}: {e}\")\n",
        "\n",
        "    logger.info(f\"Prepared {len(items)} items (skipped {sum(skip_reasons.values())})\")\n",
        "\n",
        "    if skip_reasons:\n",
        "        logger.info(\"Skip reasons:\")\n",
        "        for reason, count in skip_reasons.most_common(5):\n",
        "            logger.info(f\"  {reason}: {count}\")\n",
        "\n",
        "    if items:\n",
        "        domains = Counter([it.domain for it in items])\n",
        "        logger.info(\"Domain distribution:\")\n",
        "        for domain, count in sorted(domains.items(), key=lambda x: -x[1]):\n",
        "            logger.info(f\"  {domain}: {count}\")\n",
        "\n",
        "    return items"
      ],
      "metadata": {
        "id": "t2tX7E8UWAXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model wrapper\n",
        "class ModelWrapper:\n",
        "    \"\"\"Wrapper for LLM with activation extraction.\"\"\"\n",
        "\n",
        "    def __init__(self, config: dict, token: Optional[str] = None):\n",
        "        self.config = config['model']\n",
        "        self.max_length = self.config['max_length']\n",
        "        self.layer_slice = slice(config['processing']['layer_slice_start'], None)\n",
        "\n",
        "        dtype_map = {\n",
        "            \"bfloat16\": torch.bfloat16,\n",
        "            \"float16\": torch.float16,\n",
        "            \"float32\": torch.float32\n",
        "        }\n",
        "        self.dtype = dtype_map.get(self.config.get('dtype', 'bfloat16'), torch.bfloat16)\n",
        "\n",
        "        self.has_cuda = torch.cuda.is_available()\n",
        "        use_4bit = self.config['use_4bit'] and self.has_cuda\n",
        "\n",
        "        logger.info(f\"Loading tokenizer: {self.config['name']}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.config['name'],\n",
        "            use_fast=True,\n",
        "            token=token\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        logger.info(f\"Loading model: {self.config['name']}\")\n",
        "        load_kwargs = {\n",
        "            \"device_map\": \"auto\",\n",
        "            \"dtype\": self.dtype if not use_4bit else None,\n",
        "            \"token\": token\n",
        "        }\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.config['name'],\n",
        "            **load_kwargs\n",
        "        ).eval()\n",
        "\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "        logger.info(\"  Model loaded successfully\")\n",
        "        logger.info(f\"  dtype: {self.dtype}\")\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.model.parameters()).device\n",
        "\n",
        "    def build_prompt(self, question: str, context: str = \"\") -> str:\n",
        "        \"\"\"Build prompt from question and optional context.\"\"\"\n",
        "        instr = \"give a short, concrete answer (fewer than 6 words).\"\n",
        "        if context:\n",
        "            return f\"{context}\\n\\nq: {question}\\n{instr}\\na:\"\n",
        "        else:\n",
        "            return f\"q: {question}\\n{instr}\\na:\"\n",
        "\n",
        "    def _find_question_start_idx(self, prompt: str, offset_mapping: List) -> int:\n",
        "        \"\"\"Find token index where question starts.\"\"\"\n",
        "        q_char_start = 0\n",
        "        for pattern in [r'(?i)\\bq:\\s*', r'(?i)\\bquestion:\\s*']:\n",
        "            matches = list(re.finditer(pattern, prompt))\n",
        "            if matches:\n",
        "                q_char_start = matches[-1].end()\n",
        "                break\n",
        "\n",
        "        for idx, (start, end) in enumerate(offset_mapping):\n",
        "            if (end - start) > 0 and start >= q_char_start:\n",
        "                return idx\n",
        "\n",
        "        for idx, (start, end) in enumerate(offset_mapping):\n",
        "            if (end - start) > 0:\n",
        "                return idx\n",
        "\n",
        "        return 0\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def encode_and_extract_hidden_batch(self, prompts: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Extract hidden states for a batch of prompts.\n",
        "        Returns:\n",
        "            first_vecs: (batch_size, n_layers, hidden_dim)\n",
        "            last_vecs: (batch_size, n_layers, hidden_dim)\n",
        "        \"\"\"\n",
        "        enc_with_offsets = self.tokenizer(\n",
        "            prompts,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "\n",
        "        first_indices = []\n",
        "        for prompt, offset_map in zip(prompts, enc_with_offsets[\"offset_mapping\"]):\n",
        "            first_idx = self._find_question_start_idx(prompt, offset_map)\n",
        "            first_indices.append(first_idx)\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True,\n",
        "            padding=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        outputs = self.model(**enc, output_hidden_states=True)\n",
        "\n",
        "        all_hidden = outputs.hidden_states[1:]  # Skip embedding layer\n",
        "        selected_layers = list(range(len(all_hidden)))[self.layer_slice]\n",
        "        hidden_stack = torch.stack([all_hidden[i] for i in selected_layers], dim=0)\n",
        "\n",
        "        last_indices = enc[\"attention_mask\"].sum(dim=1) - 1\n",
        "\n",
        "        batch_size = len(prompts)\n",
        "        n_layers = hidden_stack.shape[0]\n",
        "        hidden_dim = hidden_stack.shape[3]\n",
        "\n",
        "        first_vecs = np.zeros((batch_size, n_layers, hidden_dim), dtype=np.float32)\n",
        "        last_vecs = np.zeros((batch_size, n_layers, hidden_dim), dtype=np.float32)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            first_vecs[b] = hidden_stack[:, b, first_indices[b], :].detach().to(torch.float32).cpu().numpy()\n",
        "            last_vecs[b] = hidden_stack[:, b, last_indices[b].item(), :].detach().to(torch.float32).cpu().numpy()\n",
        "\n",
        "        return first_vecs, last_vecs\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate_answer(self, prompt: str, max_new_tokens: int = 8) -> str:\n",
        "        \"\"\"Generate answer for a prompt.\"\"\"\n",
        "        enc = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated = full_text[len(prompt):].strip()\n",
        "\n",
        "        return postprocess_short_phrase(generated)\n",
        "\n",
        "    def evaluate_item(self, item: Item) -> EvaluationResult:\n",
        "        \"\"\"Evaluate a single item across all conditions.\"\"\"\n",
        "        prompt_base = self.build_prompt(item.question, context=\"\")\n",
        "        prompt_correct = self.build_prompt(item.question, context=item.correct_ctx)\n",
        "        prompt_incorrect = self.build_prompt(item.question, context=item.incorrect_ctx)\n",
        "\n",
        "        answer_base = self.generate_answer(prompt_base)\n",
        "        answer_correct = self.generate_answer(prompt_correct)\n",
        "        answer_incorrect = self.generate_answer(prompt_incorrect)\n",
        "\n",
        "        base_correct = gt_overlap(answer_base, item.gt_list)\n",
        "        correct_correct = gt_overlap(answer_correct, item.gt_list)\n",
        "        incorrect_correct = gt_overlap(answer_incorrect, item.gt_list)\n",
        "\n",
        "        is_flip = base_correct and not incorrect_correct\n",
        "\n",
        "        return EvaluationResult(\n",
        "            item_idx=item.metadata.get(\"source_index\", -1),\n",
        "            answer_base=answer_base,\n",
        "            answer_correct=answer_correct,\n",
        "            answer_incorrect=answer_incorrect,\n",
        "            base_correct=base_correct,\n",
        "            correct_correct=correct_correct,\n",
        "            incorrect_correct=incorrect_correct,\n",
        "            is_flip=is_flip\n",
        "        )\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def answer_logprob(self, prompt: str, answer: str) -> float:\n",
        "        \"\"\"compute average log-probability of 'answer' tokens given 'prompt'.\"\"\"\n",
        "        if not answer:\n",
        "            return 0.0\n",
        "\n",
        "        # tokenize separately to get prompt length\n",
        "        enc_prompt = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        enc_full = self.tokenizer(\n",
        "            prompt + \" \" + answer,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "\n",
        "        input_ids = enc_full[\"input_ids\"].to(self.device)\n",
        "        attn_mask = enc_full[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        # forward pass\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask\n",
        "        )\n",
        "        # logits shape (1, seq_len, vocab_size)\n",
        "        logits = outputs.logits[0]\n",
        "        logprobs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "        # how many tokens belong to the prompt\n",
        "        prompt_len = enc_prompt[\"input_ids\"].shape[1]\n",
        "\n",
        "        # indices of answer tokens in enc_full\n",
        "        answer_token_ids = input_ids[0, prompt_len:]  # 1d tensor\n",
        "\n",
        "        # for token at position t, prob is from logits[t-1]\n",
        "        lp = []\n",
        "        for i, tok_id in enumerate(answer_token_ids):\n",
        "            pos = prompt_len + i  # position of this token\n",
        "            if pos - 1 < 0 or pos - 1 >= logprobs.shape[0]:\n",
        "                continue\n",
        "            lp.append(logprobs[pos - 1, tok_id].item())\n",
        "\n",
        "        if not lp:\n",
        "            return 0.0\n",
        "        return float(np.mean(lp))"
      ],
      "metadata": {
        "id": "H0pXSrbAWKqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# statistics\n",
        "class WelfordAccumulator:\n",
        "    \"\"\"Streaming mean and variance computation using Welford's algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, n_layers: int, hidden_dim: int):\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Position 0: first token, Position 1: last token\n",
        "        self.n = np.zeros((n_layers, 2), dtype=np.int64)\n",
        "        self.mu = np.zeros((n_layers, 2, hidden_dim), dtype=np.float64)\n",
        "        self.M2 = np.zeros((n_layers, 2, hidden_dim), dtype=np.float64)\n",
        "\n",
        "    def update(self, first_vec: np.ndarray, last_vec: np.ndarray):\n",
        "        \"\"\"Update statistics with new observation.\"\"\"\n",
        "        for pos, vec in enumerate([first_vec, last_vec]):\n",
        "            x = vec.astype(np.float64)\n",
        "            self.n[:, pos] += 1\n",
        "            delta = x - self.mu[:, pos, :]\n",
        "            self.mu[:, pos, :] += delta / self.n[:, pos][:, None]\n",
        "            delta2 = x - self.mu[:, pos, :]\n",
        "            self.M2[:, pos, :] += delta * delta2\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Compute final mean and standard deviation.\"\"\"\n",
        "        variance = np.divide(\n",
        "            self.M2,\n",
        "            np.clip(self.n[:, :, None] - 1, 1, None)\n",
        "        )\n",
        "        std = np.sqrt(variance)\n",
        "\n",
        "        return {\n",
        "            'mu': self.mu.astype(np.float32),\n",
        "            'std': std.astype(np.float32),\n",
        "            'n': self.n.copy(),\n",
        "            'n_layers': self.n_layers,\n",
        "            'hidden_dim': self.hidden_dim\n",
        "        }\n",
        "\n",
        "def probability_integral_transform(\n",
        "    vec: np.ndarray,\n",
        "    mu: np.ndarray,\n",
        "    std: np.ndarray,\n",
        "    eps: float = 1e-8\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Apply Probability Integral Transform (PIT).\"\"\"\n",
        "    z = (vec - mu) / (std + eps)\n",
        "    u = 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n",
        "    return u\n",
        "\n",
        "def ks_statistic_uniform(u: np.ndarray) -> float:\n",
        "    \"\"\"Compute KS statistic vs Uniform(0,1).\"\"\"\n",
        "    u = np.clip(np.asarray(u, dtype=float).ravel(), 0.0, 1.0)\n",
        "    if len(u) == 0:\n",
        "        return np.nan\n",
        "    return float(spstats.kstest(u, 'uniform').statistic)\n",
        "\n",
        "def mean_absolute_deviation(u: np.ndarray, center: float = 0.5) -> float:\n",
        "    \"\"\"Compute mean absolute deviation from center.\"\"\"\n",
        "    u = np.asarray(u, dtype=float).ravel()\n",
        "    if len(u) == 0:\n",
        "        return np.nan\n",
        "    return float(np.mean(np.abs(u - center)))\n",
        "\n",
        "def tail_weight(u: np.ndarray, threshold: float = 0.01) -> float:\n",
        "    \"\"\"Compute fraction of values in tails.\"\"\"\n",
        "    u = np.clip(np.asarray(u, dtype=float).ravel(), 0.0, 1.0)\n",
        "    if len(u) == 0:\n",
        "        return np.nan\n",
        "    return float(((u <= threshold) | (u >= 1.0 - threshold)).mean())\n",
        "\n",
        "def compute_itemwise_statistics(\n",
        "    U: np.ndarray,\n",
        "    metric: str = \"ks\",\n",
        "    threshold: float = 0.01\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Compute statistics per item.\n",
        "    Args:\n",
        "        U: Array of shape (n_layers, n_items, hidden_dim)\n",
        "        metric: One of \"ks\", \"mad\", \"tail\"\n",
        "        threshold: Threshold for tail metric\n",
        "    Returns:\n",
        "        stats: Array of shape (n_layers, n_items)\n",
        "    \"\"\"\n",
        "    n_layers, n_items, hidden_dim = U.shape\n",
        "    stats = np.zeros((n_layers, n_items), dtype=float)\n",
        "\n",
        "    for i in range(n_layers):\n",
        "        for j in range(n_items):\n",
        "            u = U[i, j, :]\n",
        "            if metric == \"ks\":\n",
        "                stats[i, j] = ks_statistic_uniform(u)\n",
        "            elif metric == \"mad\":\n",
        "                stats[i, j] = mean_absolute_deviation(u)\n",
        "            elif metric == \"tail\":\n",
        "                stats[i, j] = tail_weight(u, threshold)\n",
        "\n",
        "    return stats\n",
        "\n",
        "def neuron_extremeness(U: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    compute 'extremeness' scores for PIT activations:\n",
        "    for a single PIT value u, the KS statistic vs Uniform(0,1)\n",
        "    with one sample is max(u, 1 - u).\n",
        "    here we apply that to every activation.\n",
        "\n",
        "    U shape: (n_layers, n_items, hidden_dim)\n",
        "    returns array of the same shape.\n",
        "    \"\"\"\n",
        "    U_clipped = np.clip(U, 0.0, 1.0)\n",
        "    return np.maximum(U_clipped, 1.0 - U_clipped)\n",
        "\n",
        "\n",
        "def select_top_k_neurons_globally(\n",
        "    U_incorrect: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    k: int = 100,\n",
        "    layer_indices: Optional[List[int]] = None,\n",
        ") -> List[Tuple[int, int, float]]:\n",
        "    \"\"\"\n",
        "    instead of averaging over layers, find the k most discriminative\n",
        "    individual neurons across (a subset of) layers.\n",
        "\n",
        "    U_incorrect: (n_layers, n_items, hidden_dim)  # PIT values for incorrect context\n",
        "    y:           (n_items,)  binary labels (flip=1, non-flip=0)\n",
        "    layer_indices: optional list of layer indices to search over\n",
        "                   (e.g. only last 8 layers for speed)\n",
        "\n",
        "    returns: list of (layer_idx, neuron_idx, auroc), sorted by auroc desc\n",
        "    \"\"\"\n",
        "    n_layers, n_items, hidden_dim = U_incorrect.shape\n",
        "\n",
        "    if layer_indices is None:\n",
        "        layer_indices = list(range(n_layers))\n",
        "\n",
        "    # precompute extremeness scores for all activations\n",
        "    U_ext = neuron_extremeness(U_incorrect)  # same shape\n",
        "\n",
        "    neuron_aurocs: List[Tuple[int, int, float]] = []\n",
        "\n",
        "    for layer in layer_indices:\n",
        "        layer_scores = U_ext[layer]  # shape (n_items, hidden_dim)\n",
        "        for neuron in range(hidden_dim):\n",
        "            s = layer_scores[:, neuron]\n",
        "\n",
        "            # skip completely constant neurons\n",
        "            if np.allclose(s, s[0]):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                auc = roc_auc_score(y, s)\n",
        "                neuron_aurocs.append((layer, neuron, float(auc)))\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    neuron_aurocs.sort(key=lambda t: t[2], reverse=True)\n",
        "    top_neurons = neuron_aurocs[:k]\n",
        "\n",
        "    if top_neurons:\n",
        "        l0, n0, a0 = top_neurons[0]\n",
        "        logger.info(\n",
        "            f\"top neuron (global): layer {l0}, neuron {n0}, AUROC={a0:.3f} \"\n",
        "            f\"(search over {len(layer_indices)} layers, k={k})\"\n",
        "        )\n",
        "    else:\n",
        "        logger.warning(\"no valid neurons found in select_top_k_neurons_globally\")\n",
        "\n",
        "    return top_neurons\n",
        "\n",
        "\n",
        "def build_neuron_features(\n",
        "    U: np.ndarray,\n",
        "    selected_neurons: List[Tuple[int, int, float]],\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    build a feature matrix from selected (layer, neuron) pairs.\n",
        "\n",
        "    U: PIT activations, shape (n_layers, n_items, hidden_dim)\n",
        "    selected_neurons: list of (layer_idx, neuron_idx, auroc)\n",
        "\n",
        "    returns:\n",
        "        X: (n_items, len(selected_neurons))  # each column = extremeness of one neuron\n",
        "    \"\"\"\n",
        "    n_layers, n_items, hidden_dim = U.shape\n",
        "    U_ext = neuron_extremeness(U)  # reuse same extremeness definition\n",
        "\n",
        "    X = np.zeros((n_items, len(selected_neurons)), dtype=np.float32)\n",
        "    for col, (layer, neuron, _) in enumerate(selected_neurons):\n",
        "        X[:, col] = U_ext[layer, :, neuron]\n",
        "\n",
        "    return X\n",
        "\n"
      ],
      "metadata": {
        "id": "V9LDZVkrWaUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation extraction\n",
        "def extract_pit_values(\n",
        "    items: List[Item],\n",
        "    condition: str,\n",
        "    model_wrapper: ModelWrapper,\n",
        "    baseline_stats: Dict,\n",
        "    batch_size: int = 8\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Extract PIT-transformed values for a condition.\n",
        "    Args:\n",
        "        items: List of items to process\n",
        "        condition: One of \"base\", \"correct\", \"incorrect\"\n",
        "        model_wrapper: Model wrapper instance\n",
        "        baseline_stats: Baseline statistics dict\n",
        "        batch_size: Batch size for processing\n",
        "    Returns:\n",
        "        U_last: Array of shape (n_layers, n_items, hidden_dim)\n",
        "    \"\"\"\n",
        "    prompts = []\n",
        "    for item in items:\n",
        "        if condition == \"base\":\n",
        "            prompt = model_wrapper.build_prompt(item.question, context=\"\")\n",
        "        elif condition == \"correct\":\n",
        "            prompt = model_wrapper.build_prompt(item.question, context=item.correct_ctx)\n",
        "        else:  # incorrect\n",
        "            prompt = model_wrapper.build_prompt(item.question, context=item.incorrect_ctx)\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    n_items = len(prompts)\n",
        "    n_layers = baseline_stats['n_layers']\n",
        "    hidden_dim = baseline_stats['hidden_dim']\n",
        "\n",
        "    # Only store last token\n",
        "    U_last = np.zeros((n_layers, n_items, hidden_dim), dtype=np.float32)\n",
        "\n",
        "    for start_idx in tqdm(range(0, n_items, batch_size), desc=f\"Extract {condition}\"):\n",
        "        end_idx = min(start_idx + batch_size, n_items)\n",
        "        batch_prompts = prompts[start_idx:end_idx]\n",
        "\n",
        "        # Extract hidden states\n",
        "        _, last_vecs = model_wrapper.encode_and_extract_hidden_batch(batch_prompts)\n",
        "        # last_vecs: (batch_size, n_layers, hidden_dim)\n",
        "        # Transpose to (n_layers, batch_size, hidden_dim)\n",
        "        last_vecs = np.transpose(last_vecs, (1, 0, 2))\n",
        "\n",
        "        # Apply PIT transformation\n",
        "        u_last = probability_integral_transform(\n",
        "            last_vecs,\n",
        "            baseline_stats['mu'][:, 1, :][:, None, :],  # Last token baseline\n",
        "            baseline_stats['std'][:, 1, :][:, None, :]\n",
        "        )\n",
        "\n",
        "        U_last[:, start_idx:end_idx, :] = u_last\n",
        "\n",
        "    return U_last"
      ],
      "metadata": {
        "id": "4EiGPT7_WiQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering\n",
        "def compute_basic_features(\n",
        "    U_base: np.ndarray,\n",
        "    U_correct: np.ndarray,\n",
        "    U_incorrect: np.ndarray\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Compute basic delta-delta features.\n",
        "    Returns dict with keys like \"deltadelta_ks\", each value is (n_layers, n_items)\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    for metric_name, threshold in [(\"ks\", None), (\"mad\", None),\n",
        "                                   (\"tail01\", 0.01), (\"tail05\", 0.05)]:\n",
        "        metric = \"ks\" if metric_name == \"ks\" else \"mad\" if metric_name == \"mad\" else \"tail\"\n",
        "        thr = threshold if threshold else 0.01\n",
        "\n",
        "        stat_base = compute_itemwise_statistics(U_base, metric, thr)\n",
        "        stat_correct = compute_itemwise_statistics(U_correct, metric, thr)\n",
        "        stat_incorrect = compute_itemwise_statistics(U_incorrect, metric, thr)\n",
        "\n",
        "        delta_incorrect = stat_incorrect - stat_base\n",
        "        delta_correct = stat_correct - stat_base\n",
        "        delta_delta = delta_incorrect - delta_correct\n",
        "\n",
        "        features[f\"deltadelta_{metric_name}\"] = delta_delta\n",
        "\n",
        "    return features\n",
        "\n",
        "def compute_enhanced_features(\n",
        "    U_base: np.ndarray,\n",
        "    U_correct: np.ndarray,\n",
        "    U_incorrect: np.ndarray,\n",
        "    last_k: int = 16\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Compute enhanced feature matrix for contrastive learning.\n",
        "    Returns:\n",
        "        X: Array of shape (n_items, n_features)\n",
        "           where n_features = 7 * last_k (7 feature types across last k layers)\n",
        "    \"\"\"\n",
        "    logger.info(\"Computing enhanced features...\")\n",
        "\n",
        "    # KS statistics per item\n",
        "    logger.info(\"  - KS statistics\")\n",
        "    def ks_uniform_itemwise(U: np.ndarray) -> np.ndarray:\n",
        "        n_layers, n_items, hidden_dim = U.shape\n",
        "        stats = np.zeros((n_layers, n_items), dtype=float)\n",
        "        for i in range(n_layers):\n",
        "            for j in range(n_items):\n",
        "                u = np.clip(U[i, j, :], 0.0, 1.0)\n",
        "                stats[i, j] = spstats.kstest(u, 'uniform').statistic\n",
        "        return stats\n",
        "\n",
        "    KS_b = ks_uniform_itemwise(U_base)\n",
        "    KS_w = ks_uniform_itemwise(U_incorrect)\n",
        "    KS_c = ks_uniform_itemwise(U_correct)\n",
        "\n",
        "    # Tail and MAD features\n",
        "    logger.info(\"  - Tail weights and MAD\")\n",
        "    def tail_feats_itemwise(U: np.ndarray, thr: float):\n",
        "        L, n, d = U.shape\n",
        "        tails = ((U <= thr) | (U >= (1.0 - thr))).mean(axis=2)\n",
        "        mad = np.abs(U - 0.5).mean(axis=2)\n",
        "        return tails, mad\n",
        "\n",
        "    tail01_b, mad_b = tail_feats_itemwise(U_base, 0.01)\n",
        "    tail01_w, mad_w = tail_feats_itemwise(U_incorrect, 0.01)\n",
        "    tail01_c, mad_c = tail_feats_itemwise(U_correct, 0.01)\n",
        "\n",
        "    # Variance and skewness\n",
        "    logger.info(\"  - Variance and skewness\")\n",
        "    def moments_itemwise(U):\n",
        "        L, n, d = U.shape\n",
        "        var = np.var(U, axis=2)\n",
        "        skew = spstats.skew(U, axis=2)\n",
        "        return var, skew\n",
        "\n",
        "    var_b, skew_b = moments_itemwise(U_base)\n",
        "    var_w, skew_w = moments_itemwise(U_incorrect)\n",
        "    var_c, skew_c = moments_itemwise(U_correct)\n",
        "\n",
        "    # Quantile ranges\n",
        "    logger.info(\"  - Quantile ranges\")\n",
        "    def quantile_feats(U):\n",
        "        L, n, d = U.shape\n",
        "        q01 = np.percentile(U, 1, axis=2)\n",
        "        q99 = np.percentile(U, 99, axis=2)\n",
        "        return q99 - q01\n",
        "\n",
        "    range_b = quantile_feats(U_base)\n",
        "    range_w = quantile_feats(U_incorrect)\n",
        "    range_c = quantile_feats(U_correct)\n",
        "\n",
        "    # Delta-delta features\n",
        "    logger.info(\"  - Computing delta-delta\")\n",
        "    dKS_w = KS_w - KS_b\n",
        "    dKS_c = KS_c - KS_b\n",
        "    ddKS = dKS_w - dKS_c\n",
        "\n",
        "    dd_mad = (mad_w - mad_b) - (mad_c - mad_b)\n",
        "    dd_tail01 = (tail01_w - tail01_b) - (tail01_c - tail01_b)\n",
        "    dd_var = (var_w - var_b) - (var_c - var_b)\n",
        "    dd_skew = (skew_w - skew_b) - (skew_c - skew_b)\n",
        "    dd_range = (range_w - range_b) - (range_c - range_b)\n",
        "\n",
        "    # Stack from last k layers\n",
        "    logger.info(f\"  - Stacking last {last_k} layers\")\n",
        "    mats = []\n",
        "    for M in [dKS_w, ddKS, dd_mad, dd_tail01, dd_var, dd_skew, dd_range]:\n",
        "        L = M.shape[0]\n",
        "        start = max(0, L - last_k)\n",
        "        mats.append(M[start:, :].T)  # (n_items, last_k)\n",
        "\n",
        "    X = np.concatenate(mats, axis=1).astype(np.float32)\n",
        "\n",
        "    logger.info(f\"Feature matrix: {X.shape}\")\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "1JGvIy_8WrtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contrastive learning\n",
        "class ProjectionHead(nn.Module):\n",
        "    \"\"\"Projection head for contrastive learning.\"\"\"\n",
        "\n",
        "    def __init__(self, d_in: int, d_h: int = 256, d_out: int = 64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_in, d_h),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(d_h, d_out),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        z = F.normalize(z, p=2, dim=-1)\n",
        "        return z\n",
        "\n",
        "def supervised_contrastive_loss(z: torch.Tensor, y: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
        "    \"\"\"Supervised contrastive loss.\"\"\"\n",
        "    # Compute similarity matrix\n",
        "    sim = (z @ z.t()) / temperature\n",
        "\n",
        "    # Create positive mask\n",
        "    y = y.view(-1, 1)\n",
        "    mask_pos = (y == y.t()).float()\n",
        "    eye = torch.eye(z.size(0), device=z.device)\n",
        "    mask_pos = mask_pos - eye  # Remove self-similarity\n",
        "\n",
        "    # Numerical stability\n",
        "    sim = sim - sim.max(dim=1, keepdim=True)[0].detach()\n",
        "\n",
        "    # Compute log probabilities\n",
        "    exp_sim = torch.exp(sim) * (1.0 - eye)\n",
        "    denom = exp_sim.sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
        "    log_prob = sim - torch.log(denom)\n",
        "\n",
        "    # Average over positives\n",
        "    num_pos = mask_pos.sum(dim=1).clamp_min(1.0)\n",
        "    loss = -(mask_pos * log_prob).sum(dim=1) / num_pos\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "def train_contrastive_model(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    config: dict,\n",
        "    device: torch.device\n",
        ") -> ProjectionHead:\n",
        "    \"\"\"Train contrastive projection head on training data.\"\"\"\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_train_std = scaler.fit_transform(X_train).astype(np.float32)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = torch.utils.data.TensorDataset(\n",
        "        torch.from_numpy(X_train_std),\n",
        "        torch.from_numpy(y_train.astype(np.int64))\n",
        "    )\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    proj = ProjectionHead(\n",
        "        d_in=X_train_std.shape[1],\n",
        "        d_h=config['hidden_dim'],\n",
        "        d_out=config['projection_dim']\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        proj.parameters(),\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    logger.info(f\"Training contrastive model for {config['epochs']} epochs...\")\n",
        "    proj.train()\n",
        "\n",
        "    best_train_auroc = 0\n",
        "\n",
        "    for epoch in range(1, config['epochs'] + 1):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            z = proj(xb)\n",
        "            loss = supervised_contrastive_loss(z, yb, temperature=config['temperature'])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        # Monitor progress\n",
        "        if epoch % 10 == 0:\n",
        "            avg_loss = epoch_loss / len(dataset)\n",
        "\n",
        "            # Quick evaluation\n",
        "            proj.eval()\n",
        "            with torch.no_grad():\n",
        "                Z_train = proj(torch.from_numpy(X_train_std).to(device)).cpu().numpy()\n",
        "                Z_train = Z_train / (np.linalg.norm(Z_train, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "                proto0 = Z_train[y_train == 0].mean(axis=0)\n",
        "                proto1 = Z_train[y_train == 1].mean(axis=0)\n",
        "                proto0 /= (np.linalg.norm(proto0) + 1e-12)\n",
        "                proto1 /= (np.linalg.norm(proto1) + 1e-12)\n",
        "\n",
        "                scores = (Z_train @ proto1) - (Z_train @ proto0)\n",
        "                try:\n",
        "                    train_auroc = roc_auc_score(y_train, scores)\n",
        "                    best_train_auroc = max(best_train_auroc, train_auroc)\n",
        "                    logger.info(f\"  Epoch {epoch:02d}/{config['epochs']}  \"\n",
        "                              f\"Loss: {avg_loss:.4f}  Train AUROC: {train_auroc:.3f}\")\n",
        "                except:\n",
        "                    logger.info(f\"  Epoch {epoch:02d}/{config['epochs']}  Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            proj.train()\n",
        "\n",
        "    proj.eval()\n",
        "    with torch.no_grad():\n",
        "        Z_train = proj(torch.from_numpy(X_train_std).to(device)).cpu().numpy()\n",
        "    Z_train = Z_train / (np.linalg.norm(Z_train, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "    proto0 = Z_train[y_train == 0].mean(axis=0)\n",
        "    proto1 = Z_train[y_train == 1].mean(axis=0)\n",
        "    proto0 /= (np.linalg.norm(proto0) + 1e-12)\n",
        "    proto1 /= (np.linalg.norm(proto1) + 1e-12)\n",
        "\n",
        "    return proj, scaler, proto0, proto1\n",
        "\n",
        "def evaluate_contrastive_model(\n",
        "    proj: ProjectionHead,\n",
        "    scaler: StandardScaler,\n",
        "    X_test: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    proto0: np.ndarray,\n",
        "    proto1: np.ndarray,\n",
        "    device: torch.device\n",
        ") -> Tuple[np.ndarray, Dict[str, float]]:\n",
        "\n",
        "    # standardize test features\n",
        "    X_test_std = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "    # get embeddings\n",
        "    proj.eval()\n",
        "    with torch.no_grad():\n",
        "        Z_test = proj(torch.from_numpy(X_test_std).to(device)).cpu().numpy()\n",
        "    Z_test = Z_test / (np.linalg.norm(Z_test, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "    # prototype scores\n",
        "    scores = (Z_test @ proto1) - (Z_test @ proto0)\n",
        "\n",
        "    # debug: check scores\n",
        "    logger.info(\"\\n=== DEBUGGING CONTRASTIVE SCORES ===\")\n",
        "    logger.info(f\"scores shape: {scores.shape}\")\n",
        "    logger.info(f\"scores contains NaN: {np.isnan(scores).any()}\")\n",
        "    logger.info(f\"scores contains Inf: {np.isinf(scores).any()}\")\n",
        "    if not np.isnan(scores).all():\n",
        "        logger.info(\n",
        "            f\"scores min/max/mean: \"\n",
        "            f\"{np.nanmin(scores):.6f} / {np.nanmax(scores):.6f} / {np.nanmean(scores):.6f}\"\n",
        "        )\n",
        "\n",
        "    metrics: Dict[str, float] = {}\n",
        "\n",
        "    try:\n",
        "        auroc = roc_auc_score(y_test, scores)\n",
        "        metrics['auroc'] = auroc\n",
        "\n",
        "        ci_low, ci_high = bootstrap_auroc_ci(\n",
        "            y_test,\n",
        "            scores,\n",
        "            n_resamples=CONFIG['bootstrap']['n_resamples'],\n",
        "            confidence_level=CONFIG['bootstrap']['confidence_level'],\n",
        "            seed=CONFIG['seed'],\n",
        "        )\n",
        "\n",
        "        metrics['auroc_ci_low'] = ci_low\n",
        "        metrics['auroc_ci_high'] = ci_high\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not compute AUROC or its CI: {e}\")\n",
        "        metrics['auroc'] = np.nan\n",
        "        metrics['auroc_ci_low'] = np.nan\n",
        "        metrics['auroc_ci_high'] = np.nan\n",
        "\n",
        "    return scores, metrics"
      ],
      "metadata": {
        "id": "3v2w6PHEW228"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization\n",
        "def plot_qq_uniform(u: np.ndarray, title: str, save_path: Optional[Path] = None):\n",
        "    \"\"\"Plot QQ plot against uniform distribution.\"\"\"\n",
        "    u = np.clip(np.asarray(u, dtype=float).ravel(), 0, 1)\n",
        "\n",
        "    if len(u) == 0 or np.std(u) < 1e-10:\n",
        "        logger.warning(f\"Skipping QQ plot for {title} - insufficient data\")\n",
        "        return\n",
        "\n",
        "    q_theoretical = np.linspace(0, 1, 200)\n",
        "    q_empirical = np.quantile(u, q_theoretical)\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.plot(q_theoretical, q_empirical, 'o-', markersize=3, label=\"Empirical\", alpha=0.7)\n",
        "    plt.plot([0, 1], [0, 1], '--', color='red', alpha=0.6, label=\"Uniform(0,1)\")\n",
        "    plt.xlabel(\"Theoretical quantiles\")\n",
        "    plt.ylabel(\"Empirical quantiles\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        logger.info(f\"Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_score_distributions(\n",
        "    scores: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    threshold: Optional[float] = None,\n",
        "    title: str = \"Score Distributions\",\n",
        "    xlabel: str = \"Score\",\n",
        "    save_path: Optional[Path] = None\n",
        "):\n",
        "    \"\"\"Plot score distributions for positive and negative classes.\"\"\"\n",
        "    plt.figure(figsize=(7, 4))\n",
        "\n",
        "    plt.hist(scores[labels == 0], bins=40, density=True, alpha=0.6,\n",
        "             label=f\"Non-flip (n={(labels == 0).sum()})\", color='C0')\n",
        "    plt.hist(scores[labels == 1], bins=40, density=True, alpha=0.6,\n",
        "             label=f\"Flip (n={(labels == 1).sum()})\", color='C3')\n",
        "\n",
        "    if threshold is not None:\n",
        "        plt.axvline(threshold, linestyle='--', linewidth=2, color='red',\n",
        "                   label=f\"Threshold = {threshold:.4f}\")\n",
        "\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        logger.info(f\"Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_auroc_by_layer(\n",
        "    auroc_results: Dict[str, np.ndarray],\n",
        "    save_path: Optional[Path] = None\n",
        "):\n",
        "    \"\"\"Plot AUROC by layer for different features.\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    for feat_name, aurocs in auroc_results.items():\n",
        "        label = feat_name.replace(\"deltadelta_\", \"ΔΔ \")\n",
        "        plt.plot(aurocs, marker='o', markersize=4, label=label, alpha=0.8)\n",
        "\n",
        "    plt.axhline(0.5, linestyle='--', color='black', alpha=0.3, label=\"Chance\")\n",
        "    plt.xlabel(\"Layer Index\")\n",
        "    plt.ylabel(\"AUROC\")\n",
        "    plt.title(\"Flip Detection Performance by Layer\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        logger.info(f\"Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_roc_curve_with_ci(\n",
        "    y_true: np.ndarray,\n",
        "    scores: np.ndarray,\n",
        "    title: str = \"ROC Curve\",\n",
        "    save_path: Optional[Path] = None\n",
        "):\n",
        "    \"\"\"Plot ROC curve.\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
        "    auroc = roc_auc_score(y_true, scores)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=f'AUROC = {auroc:.3f}')\n",
        "    plt.plot([0, 1], [0, 1], '--', color='gray', alpha=0.5, label='Chance')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        logger.info(f\"Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "S2L_wt2bXHnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create output directory\n",
        "output_dir = Path(\"/content/results\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "logger.info(f\"Output directory: {output_dir}\")"
      ],
      "metadata": {
        "id": "MsxvSMJRXRX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# authentication\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"HUGGINGFACE AUTHENTICATION\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import login, HfFolder\n",
        "\n",
        "    hf_token = HfFolder.get_token()\n",
        "\n",
        "    if hf_token:\n",
        "        logger.info(\"HuggingFace token found\")\n",
        "    else:\n",
        "        logger.info(\"No token found. Please login:\")\n",
        "        login()\n",
        "        hf_token = HfFolder.get_token()\n",
        "\n",
        "    if not hf_token:\n",
        "        raise RuntimeError(\"HuggingFace authentication required for Llama models\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Authentication failed: {e}\")\n",
        "    logger.info(\"Please set your token with: from huggingface_hub import login; login()\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "pbNWadr2XaT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"DATA LOADING\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    logger.info(\"Upload ConflictQA JSON file\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        json_path = next(iter(uploaded.keys()))\n",
        "        logger.info(f\"File uploaded: {json_path}\")\n",
        "\n",
        "        # Read JSON\n",
        "        try:\n",
        "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                raw = json.load(f)\n",
        "            if not isinstance(raw, list):\n",
        "                raw = [raw]\n",
        "        except json.JSONDecodeError:\n",
        "            logger.info(\"Trying JSONL format...\")\n",
        "            raw = []\n",
        "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    if line.strip():\n",
        "                        raw.append(json.loads(line))\n",
        "\n",
        "        logger.info(f\"Read {len(raw)} raw items\")\n",
        "\n",
        "        items = build_items_from_raw(raw)\n",
        "\n",
        "        if items:\n",
        "            random.shuffle(items)\n",
        "            logger.info(f\"Loaded {len(items)} valid items from upload\")\n",
        "        else:\n",
        "            logger.warning(\"No valid items found, using demo data\")\n",
        "    else:\n",
        "        logger.info(\"No file uploaded\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Upload failed: {e}\")\n",
        "\n",
        "logger.info(f\"\\nTotal items available: {len(items)}\")\n",
        "\n",
        "# Sample if needed\n",
        "n_items_total = CONFIG['data']['n_items_total']\n",
        "if len(items) > n_items_total:\n",
        "    items = random.sample(items, n_items_total)\n",
        "    logger.info(f\"Sampled {len(items)} items\")\n",
        "\n",
        "if len(items) == 0:\n",
        "    raise ValueError(\"No items to process\")"
      ],
      "metadata": {
        "id": "Vc8X-Cl8XhQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model initialization\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"MODEL INITIALIZATION\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "model_wrapper = ModelWrapper(CONFIG, token=hf_token)\n",
        "\n",
        "# Get model dimensions\n",
        "prompt = model_wrapper.build_prompt(items[0].question, context=\"\")\n",
        "_, last_vec = model_wrapper.encode_and_extract_hidden_batch([prompt])\n",
        "n_layers, hidden_dim = last_vec.shape[1], last_vec.shape[2]\n",
        "\n",
        "logger.info(f\"  Model dimensions: {n_layers} layers, {hidden_dim} hidden dim\")"
      ],
      "metadata": {
        "id": "IhFFD2mkXuxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label computation (for stratified split)\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"COMPUTING LABELS FOR STRATIFICATION\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "eval_results = []\n",
        "for item in tqdm(items, desc=\"Evaluating items\"):\n",
        "    result = model_wrapper.evaluate_item(item)\n",
        "    eval_results.append(result)\n",
        "\n",
        "y_all = np.array([int(r.is_flip) for r in eval_results])\n",
        "\n",
        "n_flips = y_all.sum()\n",
        "n_total = len(y_all)\n",
        "\n",
        "logger.info(f\"  Labels computed:\")\n",
        "logger.info(f\"  Total items: {n_total}\")\n",
        "logger.info(f\"  Flips (y=1): {n_flips} ({100*n_flips/n_total:.1f}%)\")\n",
        "logger.info(f\"  Non-flips (y=0): {n_total - n_flips} ({100*(n_total-n_flips)/n_total:.1f}%)\")\n",
        "\n",
        "if n_flips < 10 or (n_total - n_flips) < 10:\n",
        "    logger.warning(\"Very few samples in one class\")"
      ],
      "metadata": {
        "id": "-8UNNG08X1C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"TRAIN/TEST SPLIT (STRATIFIED)\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "train_idx, test_idx = train_test_split(\n",
        "    np.arange(len(items)),\n",
        "    test_size=CONFIG['data']['test_size'],\n",
        "    stratify=y_all,\n",
        "    random_state=CONFIG['data']['random_state']\n",
        ")\n",
        "\n",
        "train_items = [items[i] for i in train_idx]\n",
        "test_items = [items[i] for i in test_idx]\n",
        "\n",
        "y_train = y_all[train_idx]\n",
        "y_test = y_all[test_idx]\n",
        "\n",
        "train_results = [eval_results[i] for i in train_idx]\n",
        "test_results = [eval_results[i] for i in test_idx]\n",
        "\n",
        "logger.info(f\"  Data split:\")\n",
        "logger.info(f\"  Train: {len(train_items)} items ({y_train.sum()} flips, {100*y_train.mean():.1f}%)\")\n",
        "logger.info(f\"  Test:  {len(test_items)} items ({y_test.sum()} flips, {100*y_test.mean():.1f}%)\")\n",
        "\n",
        "# Sanity check\n",
        "assert len(set(train_idx) & set(test_idx)) == 0, \"Train/test overlap detected\"\n",
        "logger.info(\"  No train/test overlap confirmed\")\n",
        "\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"CONFIDENCE-BASED BASELINE (INCORRECT CONTEXT, TEST SET)\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "conf_scores = []\n",
        "y_conf = []\n",
        "\n",
        "for item, res in zip(test_items, test_results):\n",
        "    prompt_incorrect = model_wrapper.build_prompt(item.question, context=item.incorrect_ctx)\n",
        "    # reuse the already generated answer_incorrect from res\n",
        "    answer_incorrect = res.answer_incorrect\n",
        "\n",
        "    conf = model_wrapper.answer_logprob(prompt_incorrect, answer_incorrect)\n",
        "    conf_scores.append(conf)\n",
        "    y_conf.append(int(res.is_flip))\n",
        "\n",
        "conf_scores = np.array(conf_scores, dtype=float)\n",
        "y_conf = np.array(y_conf, dtype=int)\n",
        "\n",
        "# flips should have lower confidence\n",
        "try:\n",
        "    auroc_conf = roc_auc_score(y_conf, -conf_scores)\n",
        "except Exception as e:\n",
        "    logger.warning(f\"could not compute AUROC for confidence baseline: {e}\")\n",
        "    auroc_conf = np.nan\n",
        "\n",
        "logger.info(f\"  confidence baseline AUROC (flip vs non-flip, TEST): {auroc_conf:.3f}\")"
      ],
      "metadata": {
        "id": "tWgjv0-sYB4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline statistics (TRAIN only)\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"COMPUTING BASELINE STATISTICS (TRAIN SET ONLY)\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "accumulator = WelfordAccumulator(n_layers, hidden_dim)\n",
        "\n",
        "n_baseline = min(CONFIG['data']['n_baseline_items'], len(train_items))\n",
        "logger.info(f\"Using {n_baseline} training items for baseline\")\n",
        "\n",
        "for item in tqdm(train_items[:n_baseline], desc=\"Baseline\"):\n",
        "    prompt = model_wrapper.build_prompt(item.question, context=\"\")\n",
        "    _, last_vec = model_wrapper.encode_and_extract_hidden_batch([prompt])\n",
        "    # Note: We only use last token for baseline, not first\n",
        "    accumulator.update(np.zeros_like(last_vec[0]), last_vec[0])\n",
        "\n",
        "baseline_stats = accumulator.get_statistics()\n",
        "logger.info(\"  Baseline statistics computed from TRAIN set only\")\n"
      ],
      "metadata": {
        "id": "bU5WLJWUYPsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation extraction\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"EXTRACTING ACTIVATIONS\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "batch_size = CONFIG['processing']['batch_size']\n",
        "\n",
        "# Train set\n",
        "logger.info(\"Processing TRAIN set...\")\n",
        "U_train_base = extract_pit_values(train_items, \"base\", model_wrapper, baseline_stats, batch_size)\n",
        "U_train_correct = extract_pit_values(train_items, \"correct\", model_wrapper, baseline_stats, batch_size)\n",
        "U_train_incorrect = extract_pit_values(train_items, \"incorrect\", model_wrapper, baseline_stats, batch_size)\n",
        "\n",
        "# Test set\n",
        "logger.info(\"Processing TEST set...\")\n",
        "U_test_base = extract_pit_values(test_items, \"base\", model_wrapper, baseline_stats, batch_size)\n",
        "U_test_correct = extract_pit_values(test_items, \"correct\", model_wrapper, baseline_stats, batch_size)\n",
        "U_test_incorrect = extract_pit_values(test_items, \"incorrect\", model_wrapper, baseline_stats, batch_size)\n",
        "\n",
        "logger.info(\"  Activations extracted:\")\n",
        "logger.info(f\"  Train shape: {U_train_base.shape}\")\n",
        "logger.info(f\"  Test shape:  {U_test_base.shape}\")"
      ],
      "metadata": {
        "id": "6eTFmPyEYXpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NEURON-LEVEL EXPERIMENT\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"NEURON-LEVEL FEATURES (INCORRECT CONTEXT ONLY)\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "# to keep it computationally reasonable, search only in the last K layers\n",
        "LAST_K_NEURON = 16\n",
        "L_total = U_train_incorrect.shape[0]\n",
        "layer_indices = list(range(max(0, L_total - LAST_K_NEURON), L_total))\n",
        "\n",
        "logger.info(\n",
        "    f\"searching top neurons in layers {layer_indices[0]}..{layer_indices[-1]} \"\n",
        "    f\"({len(layer_indices)} layers, hidden_dim={U_train_incorrect.shape[2]})\"\n",
        ")\n",
        "\n",
        "# 1) select top-k most discriminative neurons on TRAIN set\n",
        "TOP_K_NEURONS = 100  # you can lower this if it is too slow\n",
        "top_neurons = select_top_k_neurons_globally(\n",
        "    U_incorrect=U_train_incorrect,\n",
        "    y=y_train,\n",
        "    k=TOP_K_NEURONS,\n",
        "    layer_indices=layer_indices,\n",
        ")\n",
        "\n",
        "logger.info(f\"selected {len(top_neurons)} neurons for neuron-level probe\")\n",
        "\n",
        "if len(top_neurons) > 0:\n",
        "    # 2) build neuron-extremeness features for TRAIN and TEST\n",
        "    X_train_neuron = build_neuron_features(U_train_incorrect, top_neurons)\n",
        "    X_test_neuron = build_neuron_features(U_test_incorrect, top_neurons)\n",
        "\n",
        "    logger.info(f\"neuron-level feature shapes: train={X_train_neuron.shape}, test={X_test_neuron.shape}\")\n",
        "\n",
        "    # 3) standardize + logistic regression\n",
        "    scaler_neuron = StandardScaler()\n",
        "    X_train_std = scaler_neuron.fit_transform(X_train_neuron)\n",
        "    X_test_std = scaler_neuron.transform(X_test_neuron)\n",
        "\n",
        "    clf_neuron = LogisticRegression(\n",
        "        penalty='l2',\n",
        "        C=1.0,\n",
        "        solver='liblinear',\n",
        "        class_weight='balanced',\n",
        "        max_iter=500,\n",
        "        random_state=CONFIG['seed'],\n",
        "    )\n",
        "\n",
        "    clf_neuron.fit(X_train_std, y_train)\n",
        "    neuron_scores_test = clf_neuron.decision_function(X_test_std)\n",
        "\n",
        "    try:\n",
        "        auroc_neuron = roc_auc_score(y_test, neuron_scores_test)\n",
        "        ci_low_neuron, ci_high_neuron = bootstrap_auroc_ci(\n",
        "            y_test,\n",
        "            neuron_scores_test,\n",
        "            n_resamples=CONFIG['bootstrap']['n_resamples'],\n",
        "            confidence_level=CONFIG['bootstrap']['confidence_level'],\n",
        "            seed=CONFIG['seed'],\n",
        "        )\n",
        "\n",
        "        logger.info(\"\\n\" + \"-\"*70)\n",
        "        logger.info(\"NEURON-LEVEL PROBE RESULTS (TEST SET)\")\n",
        "        logger.info(\"-\"*70)\n",
        "        logger.info(f\"  AUROC: {auroc_neuron:.3f} [{ci_low_neuron:.3f}, {ci_high_neuron:.3f}]\")\n",
        "        logger.info(\"-\"*70)\n",
        "\n",
        "        # optional: quick histogram / roc\n",
        "        plot_score_distributions(\n",
        "            neuron_scores_test,\n",
        "            y_test,\n",
        "            title=f\"Neuron-level Scores (Test Set, AUROC={auroc_neuron:.3f})\",\n",
        "            xlabel=\"Neuron extremeness (max(u, 1-u))\",\n",
        "            save_path=output_dir / \"neuron_scores_test.png\",\n",
        "        )\n",
        "\n",
        "        plot_roc_curve_with_ci(\n",
        "            y_test,\n",
        "            neuron_scores_test,\n",
        "            title=\"Neuron-level ROC Curve (Test Set)\",\n",
        "            save_path=output_dir / \"neuron_roc_test.png\",\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"could not compute AUROC for neuron-level probe: {e}\")\n",
        "\n",
        "    # ==============================================================\n",
        "    # NEURON-LEVEL CONTRASTIVE LEARNING (using the same neuron features)\n",
        "    # ==============================================================\n",
        "    if CONFIG['contrastive']['enabled'] and len(np.unique(y_train)) >= 2:\n",
        "        logger.info(\"\\n\" + \"-\"*70)\n",
        "        logger.info(\"NEURON-LEVEL CONTRASTIVE LEARNING\")\n",
        "        logger.info(\"-\"*70)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        logger.info(f\"training neuron-level contrastive model on device: {device}\")\n",
        "\n",
        "        # re-use X_train_neuron / X_test_neuron directly\n",
        "        proj_neuron, scaler_neuron_con, proto0_neuron, proto1_neuron = train_contrastive_model(\n",
        "            X_train_neuron,\n",
        "            y_train,\n",
        "            CONFIG['contrastive'],\n",
        "            device,\n",
        "        )\n",
        "\n",
        "        # evaluate on TEST set\n",
        "        neuron_contrastive_scores_test, neuron_contrastive_metrics = evaluate_contrastive_model(\n",
        "            proj_neuron,\n",
        "            scaler_neuron_con,\n",
        "            X_test_neuron,\n",
        "            y_test,\n",
        "            proto0_neuron,\n",
        "            proto1_neuron,\n",
        "            device,\n",
        "        )\n",
        "\n",
        "        if not np.isnan(neuron_contrastive_metrics['auroc']):\n",
        "            logger.info(\n",
        "                f\"  neuron-level contrastive AUROC: \"\n",
        "                f\"{neuron_contrastive_metrics['auroc']:.4f} \"\n",
        "                f\"[{neuron_contrastive_metrics['auroc_ci_low']:.4f}, \"\n",
        "                f\"{neuron_contrastive_metrics['auroc_ci_high']:.4f}]\"\n",
        "            )\n",
        "\n",
        "            # visualize\n",
        "            plot_score_distributions(\n",
        "                neuron_contrastive_scores_test,\n",
        "                y_test,\n",
        "                title=(\n",
        "                    \"Neuron-level Contrastive Scores \"\n",
        "                    f\"(Test Set, AUROC={neuron_contrastive_metrics['auroc']:.3f})\"\n",
        "                ),\n",
        "                xlabel=\"Prototype score (cosine1 − cosine0)\",\n",
        "                save_path=output_dir / \"neuron_contrastive_scores_test.png\",\n",
        "            )\n",
        "\n",
        "            plot_roc_curve_with_ci(\n",
        "                y_test,\n",
        "                neuron_contrastive_scores_test,\n",
        "                title=\"Neuron-level Contrastive ROC Curve (Test Set)\",\n",
        "                save_path=output_dir / \"neuron_contrastive_roc_test.png\",\n",
        "            )\n",
        "\n",
        "            # simple 0-threshold classification for error analysis\n",
        "            neuron_contrastive_pred = (neuron_contrastive_scores_test > 0).astype(int)\n",
        "            neuron_contrastive_errors = analyze_misclassifications(\n",
        "                test_items, y_test, neuron_contrastive_pred, neuron_contrastive_scores_test\n",
        "            )\n",
        "            logger.info(\"\\nERROR ANALYSIS (Neuron-level Contrastive, TEST set)\")\n",
        "            logger.info(\n",
        "                f\"  false positives: {neuron_contrastive_errors['false_positives']['count']}  \"\n",
        "                f\"domains: {neuron_contrastive_errors['false_positives']['domains']}\"\n",
        "            )\n",
        "            logger.info(\n",
        "                f\"  false negatives: {neuron_contrastive_errors['false_negatives']['count']}  \"\n",
        "                f\"domains: {neuron_contrastive_errors['false_negatives']['domains']}\"\n",
        "            )\n",
        "        else:\n",
        "            logger.warning(\"  neuron-level contrastive AUROC is NaN; skipping plots\")\n",
        "\n",
        "else:\n",
        "    logger.warning(\"no neurons selected, skipping neuron-level probe\")\n"
      ],
      "metadata": {
        "id": "ZR9UBpugIZZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization (TEST set)\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"GENERATING VISUALIZATIONS (TEST SET)\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "# QQ plots for last layer\n",
        "layer_idx = -1\n",
        "plot_qq_uniform(\n",
        "    U_test_base[layer_idx].ravel(),\n",
        "    \"QQ Plot: Base Condition (Test Set, Last Layer)\",\n",
        "    output_dir / \"qq_base_test.png\"\n",
        ")\n",
        "\n",
        "plot_qq_uniform(\n",
        "    U_test_incorrect[layer_idx].ravel(),\n",
        "    \"QQ Plot: Incorrect Context (Test Set, Last Layer)\",\n",
        "    output_dir / \"qq_incorrect_test.png\"\n",
        ")\n",
        "\n",
        "# Histogram comparison\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.hist(U_test_base[layer_idx].ravel(), bins=30, density=True, alpha=0.5,\n",
        "          label=\"Base\", color='C0')\n",
        "plt.hist(U_test_correct[layer_idx].ravel(), bins=30, density=True, alpha=0.5,\n",
        "          label=\"Correct\", color='C2')\n",
        "plt.hist(U_test_incorrect[layer_idx].ravel(), bins=30, density=True, alpha=0.5,\n",
        "          label=\"Incorrect\", color='C3')\n",
        "plt.axhline(1.0, linestyle='--', color='black', alpha=0.3, label=\"Uniform(0,1)\")\n",
        "plt.xlabel(\"PIT value\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"PIT Distributions (Test Set, Last Layer)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / \"pit_histogram_test.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "u6s2jZ1FYcf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic features and baseline classifier\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"BASELINE CLASSIFIER (SIMPLE FEATURES)\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "# Compute features\n",
        "train_features = compute_basic_features(U_train_base, U_train_correct, U_train_incorrect)\n",
        "test_features = compute_basic_features(U_test_base, U_test_correct, U_test_incorrect)\n",
        "\n",
        "# Layer-wise AUROC on test set\n",
        "logger.info(\"\\nPer-layer performance (TEST SET):\")\n",
        "auroc_results_test = {}\n",
        "\n",
        "for feat_name in train_features.keys():\n",
        "    test_feat_matrix = test_features[feat_name]\n",
        "\n",
        "    n_layers_feat = test_feat_matrix.shape[0]\n",
        "    aurocs = np.full(n_layers_feat, np.nan)\n",
        "\n",
        "    for i in range(n_layers_feat):\n",
        "        if len(np.unique(y_test)) >= 2:\n",
        "            try:\n",
        "                aurocs[i] = roc_auc_score(y_test, test_feat_matrix[i, :])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    auroc_results_test[feat_name] = aurocs\n",
        "\n",
        "    best_layer = int(np.nanargmax(aurocs))\n",
        "    best_auroc = aurocs[best_layer]\n",
        "    logger.info(f\"  {feat_name:20s}: Best = Layer {best_layer+1:02d}, AUROC = {best_auroc:.3f}\")\n",
        "\n",
        "# Plot AUROC by layer\n",
        "plot_auroc_by_layer(auroc_results_test, output_dir / \"auroc_by_layer_test.png\")\n",
        "\n",
        "layer_summaries = summarize_layer_importance(auroc_results_test)\n",
        "logger.info(\"\\nLAYER-WISE INTERPRETATION (TEST SET):\")\n",
        "for line in layer_summaries:\n",
        "    logger.info(\"  \" + line)\n",
        "\n",
        "# Aggregate last layers and evaluate\n",
        "band = CONFIG['analysis']['last_layers_band']\n",
        "\n",
        "def select_top_layers_by_train_auroc(\n",
        "    train_feat_matrix: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    k: int = 8\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    select top-k layers based on AUROC computed on the TRAIN set only.\n",
        "\n",
        "    train_feat_matrix: shape (n_layers, n_items)\n",
        "    y_train:           shape (n_items,)\n",
        "    returns: sorted list of layer indices (0-based)\n",
        "    \"\"\"\n",
        "    n_layers = train_feat_matrix.shape[0]\n",
        "    aurocs = np.full(n_layers, np.nan, dtype=float)\n",
        "\n",
        "    # compute AUROC per layer on TRAIN\n",
        "    if len(np.unique(y_train)) < 2:\n",
        "        logger.warning(\"cannot select layers by AUROC: only one class in y_train\")\n",
        "        # fallback: last k layers\n",
        "        start = max(0, n_layers - k)\n",
        "        return list(range(start, n_layers))\n",
        "\n",
        "    for i in range(n_layers):\n",
        "        layer_scores = train_feat_matrix[i, :]\n",
        "        # skip constant layers\n",
        "        if np.allclose(layer_scores, layer_scores[0]):\n",
        "            continue\n",
        "        try:\n",
        "            aurocs[i] = roc_auc_score(y_train, layer_scores)\n",
        "        except Exception:\n",
        "            # keep NaN if AUROC fails\n",
        "            pass\n",
        "\n",
        "    valid_idx = np.where(~np.isnan(aurocs))[0]\n",
        "\n",
        "    if len(valid_idx) == 0:\n",
        "        logger.warning(\"no valid AUROCs for layer selection; using last k layers as fallback\")\n",
        "        start = max(0, n_layers - k)\n",
        "        return list(range(start, n_layers))\n",
        "\n",
        "    # pick top-k layers among valid indices\n",
        "    top_idx = valid_idx[np.argsort(aurocs[valid_idx])[-k:]]\n",
        "    top_idx = sorted(top_idx.tolist())\n",
        "\n",
        "    logger.info(\"Selected layers for ΔΔ MAD aggregation (by TRAIN AUROC):\")\n",
        "    logger.info(\n",
        "        \"  \" + \", \".join(f\"L{idx+1}={aurocs[idx]:.3f}\" for idx in top_idx)\n",
        "    )\n",
        "\n",
        "    return top_idx\n",
        "\n",
        "# Use MAD feature aggregated over top layers chosen on TRAIN\n",
        "train_mad_feat = train_features[\"deltadelta_mad\"]\n",
        "test_mad_feat  = test_features[\"deltadelta_mad\"]\n",
        "\n",
        "selected_layers = select_top_layers_by_train_auroc(\n",
        "    train_mad_feat,\n",
        "    y_train,\n",
        "    k=band\n",
        ")\n",
        "\n",
        "# average ΔΔ MAD over the selected layers\n",
        "train_scores = train_mad_feat[selected_layers, :].mean(axis=0)\n",
        "test_scores  = test_mad_feat[selected_layers, :].mean(axis=0)\n",
        "\n",
        "# Select threshold on TRAIN set\n",
        "train_flip_mask = (y_train == 1)\n",
        "train_neg_scores = train_scores[~train_flip_mask]\n",
        "\n",
        "if len(train_neg_scores) > 0:\n",
        "    threshold = np.percentile(train_neg_scores, CONFIG['analysis']['threshold_percentile'])\n",
        "else:\n",
        "    threshold = np.median(train_scores)\n",
        "\n",
        "logger.info(f\"\\nThreshold (selected on TRAIN): {threshold:.6f}\")\n",
        "\n",
        "# Evaluate on TEST set\n",
        "test_predictions = (test_scores > threshold).astype(int)\n",
        "\n",
        "cm = confusion_matrix(y_test, test_predictions, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, 0)\n",
        "\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
        "f1 = 2 * precision * sensitivity / (precision + sensitivity) if (precision + sensitivity) > 0 else np.nan\n",
        "\n",
        "logger.info(\"\\n=== DEBUGGING BASELINE SCORES ===\")\n",
        "logger.info(f\"test_scores shape: {test_scores.shape}\")\n",
        "logger.info(f\"test_scores contains NaN: {np.isnan(test_scores).any()}\")\n",
        "logger.info(f\"test_scores contains Inf: {np.isinf(test_scores).any()}\")\n",
        "logger.info(f\"test_scores min/max/mean: {np.nanmin(test_scores):.6f} / {np.nanmax(test_scores):.6f} / {np.nanmean(test_scores):.6f}\")\n",
        "logger.info(f\"test_scores unique values: {len(np.unique(test_scores))}\")\n",
        "logger.info(f\"y_test distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "try:\n",
        "    auroc_baseline = roc_auc_score(y_test, test_scores)\n",
        "\n",
        "    auroc_ci_low, auroc_ci_high = bootstrap_auroc_ci(\n",
        "        y_test,\n",
        "        test_scores,\n",
        "        n_resamples=CONFIG['bootstrap']['n_resamples'],\n",
        "        confidence_level=CONFIG['bootstrap']['confidence_level'],\n",
        "        seed=CONFIG['seed'],\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Could not compute AUROC or its CI: {e}\")\n",
        "    auroc_baseline = np.nan\n",
        "    auroc_ci_low = np.nan\n",
        "    auroc_ci_high = np.nan\n",
        "\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(f\"BASELINE RESULTS (ΔΔ MAD, {band}-layer aggregate, TEST SET)\")\n",
        "logger.info(\"=\"*70)\n",
        "logger.info(f\"  AUROC:       {auroc_baseline:.3f} [{auroc_ci_low:.3f}, {auroc_ci_high:.3f}]\")\n",
        "logger.info(f\"  Specificity: {specificity:.3f}\")\n",
        "logger.info(f\"  Sensitivity: {sensitivity:.3f}\")\n",
        "logger.info(f\"  Precision:   {precision:.3f}\")\n",
        "logger.info(f\"  F1 Score:    {f1:.3f}\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "# Plot distributions\n",
        "plot_score_distributions(\n",
        "    test_scores,\n",
        "    y_test,\n",
        "    threshold=threshold,\n",
        "    title=f\"Baseline Score Distributions (Test Set, AUROC={auroc_baseline:.3f})\",\n",
        "    xlabel=\"ΔΔ MAD Score\",\n",
        "    save_path=output_dir / \"baseline_scores_test.png\"\n",
        ")\n",
        "\n",
        "# ROC curve\n",
        "plot_roc_curve_with_ci(\n",
        "    y_test,\n",
        "    test_scores,\n",
        "    title=f\"Baseline ROC Curve (Test Set)\",\n",
        "    save_path=output_dir / \"baseline_roc_test.png\"\n",
        ")\n",
        "\n",
        "error_info = analyze_misclassifications(test_items, y_test, test_predictions, test_scores)\n",
        "logger.info(\"\\nERROR ANALYSIS (ΔΔ MAD baseline, TEST set)\")\n",
        "logger.info(f\"  false positives: {error_info['false_positives']['count']}  \"\n",
        "            f\"domains: {error_info['false_positives']['domains']}\")\n",
        "logger.info(f\"  false negatives: {error_info['false_negatives']['count']}  \"\n",
        "            f\"domains: {error_info['false_negatives']['domains']}\")"
      ],
      "metadata": {
        "id": "0CRU2E-UYlVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables that will be used later in results section\n",
        "metrics = None\n",
        "test_contrastive_scores = None\n",
        "ablation_results = None\n",
        "\n",
        "if CONFIG['contrastive']['enabled'] and len(np.unique(y_train)) >= 2:\n",
        "    logger.info(\"\\n\" + \"=\"*70)\n",
        "    logger.info(\"CONTRASTIVE LEARNING\")\n",
        "    logger.info(\"=\"*70)\n",
        "\n",
        "    # Build enhanced features\n",
        "    X_train = compute_enhanced_features(\n",
        "        U_train_base,\n",
        "        U_train_correct,\n",
        "        U_train_incorrect,\n",
        "        last_k=CONFIG['contrastive']['last_k_layers']\n",
        "    )\n",
        "\n",
        "    X_test = compute_enhanced_features(\n",
        "        U_test_base,\n",
        "        U_test_correct,\n",
        "        U_test_incorrect,\n",
        "        last_k=CONFIG['contrastive']['last_k_layers']\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Feature matrices:\")\n",
        "    logger.info(f\"  Train: {X_train.shape}\")\n",
        "    logger.info(f\"  Test:  {X_test.shape}\")\n",
        "\n",
        "    # Train contrastive model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Training on device: {device}\")\n",
        "\n",
        "    proj, scaler, proto0, proto1 = train_contrastive_model(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        CONFIG['contrastive'],\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    logger.info(\"\\nEvaluating on TEST set...\")\n",
        "    test_contrastive_scores, metrics = evaluate_contrastive_model(\n",
        "        proj,\n",
        "        scaler,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        proto0,\n",
        "        proto1,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Debug: Check for NaN in scores and metrics\n",
        "    logger.info(\"\\n\" + \"=\"*70)\n",
        "    logger.info(\"CONTRASTIVE LEARNING RESULTS (TEST SET)\")\n",
        "    logger.info(\"=\"*70)\n",
        "\n",
        "    # Handle potential NaN in metrics\n",
        "    if not np.isnan(metrics['auroc']):\n",
        "        logger.info(f\"  Prototype AUROC: {metrics['auroc']:.4f} \"\n",
        "                    f\"[{metrics['auroc_ci_low']:.4f}, {metrics['auroc_ci_high']:.4f}]\")\n",
        "    else:\n",
        "        logger.warning(f\"  Prototype AUROC: NaN (computation failed)\")\n",
        "        logger.warning(f\"  CI Low: {metrics['auroc_ci_low']}, CI High: {metrics['auroc_ci_high']}\")\n",
        "\n",
        "    # Improvement over baseline\n",
        "    if not np.isnan(auroc_baseline) and not np.isnan(metrics['auroc']):\n",
        "        improvement = metrics['auroc'] - auroc_baseline\n",
        "        logger.info(f\"  Improvement:     +{improvement:.4f} over baseline\")\n",
        "    elif np.isnan(metrics['auroc']):\n",
        "        logger.warning(f\"  Cannot compute improvement: contrastive AUROC is NaN\")\n",
        "    elif np.isnan(auroc_baseline):\n",
        "        logger.warning(f\"  Cannot compute improvement: baseline AUROC is NaN\")\n",
        "\n",
        "    logger.info(\"=\"*70)\n",
        "\n",
        "    # Visualize (only if there are valid scores)\n",
        "    if not np.isnan(test_contrastive_scores).all():\n",
        "        auroc_display = metrics['auroc'] if not np.isnan(metrics['auroc']) else np.nan\n",
        "\n",
        "        plot_score_distributions(\n",
        "            test_contrastive_scores,\n",
        "            y_test,\n",
        "            title=f\"Contrastive Scores (Test Set, AUROC={auroc_display:.3f})\",\n",
        "            xlabel=\"Prototype Score (cosine₁ − cosine₀)\",\n",
        "            save_path=output_dir / \"contrastive_scores_test.png\"\n",
        "        )\n",
        "\n",
        "        plot_roc_curve_with_ci(\n",
        "            y_test,\n",
        "            test_contrastive_scores,\n",
        "            title=\"Contrastive Learning ROC Curve (Test Set)\",\n",
        "            save_path=output_dir / \"contrastive_roc_test.png\"\n",
        "        )\n",
        "\n",
        "        # Error analysis\n",
        "        test_contrastive_predictions = (test_contrastive_scores > 0).astype(int)\n",
        "        contrastive_error_info = analyze_misclassifications(\n",
        "            test_items, y_test, test_contrastive_predictions, test_contrastive_scores\n",
        "        )\n",
        "        logger.info(\"\\nERROR ANALYSIS (Contrastive Learning, TEST set)\")\n",
        "        logger.info(f\"  false positives: {contrastive_error_info['false_positives']['count']}  \"\n",
        "                    f\"domains: {contrastive_error_info['false_positives']['domains']}\")\n",
        "        logger.info(f\"  false negatives: {contrastive_error_info['false_negatives']['count']}  \"\n",
        "                    f\"domains: {contrastive_error_info['false_negatives']['domains']}\")\n",
        "    else:\n",
        "        logger.warning(\"  Skipping visualization and error analysis - all scores are NaN\")\n",
        "\n",
        "    # ====================================================================\n",
        "    # STATISTICAL SIGNIFICANCE TESTING\n",
        "    # ====================================================================\n",
        "\n",
        "    if not np.isnan(test_contrastive_scores).all() and not np.isnan(metrics['auroc']):\n",
        "\n",
        "        logger.info(\"\\n\" + \"=\"*70)\n",
        "        logger.info(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
        "        logger.info(\"=\"*70)\n",
        "\n",
        "        def auroc_difference(baseline_scores, contrastive_scores, y_true):\n",
        "            \"\"\"Compute difference in AUROC between two methods.\"\"\"\n",
        "            try:\n",
        "                auroc_baseline = roc_auc_score(y_true, baseline_scores)\n",
        "                auroc_contrastive = roc_auc_score(y_true, contrastive_scores)\n",
        "                return auroc_contrastive - auroc_baseline\n",
        "            except:\n",
        "                return np.nan\n",
        "\n",
        "        # Bootstrap test for significance\n",
        "        logger.info(\"Running bootstrap test for method comparison...\")\n",
        "        rng = np.random.default_rng(CONFIG['seed'])\n",
        "        n = len(y_test)\n",
        "        diff_stats = []\n",
        "\n",
        "        for _ in tqdm(range(CONFIG['bootstrap']['n_resamples']), desc=\"Bootstrap significance\"):\n",
        "            idx = rng.choice(n, size=n, replace=True)\n",
        "            y_boot = y_test[idx]\n",
        "            baseline_boot = test_scores[idx]\n",
        "            contrastive_boot = test_contrastive_scores[idx]\n",
        "\n",
        "            # Skip degenerate resamples\n",
        "            if len(np.unique(y_boot)) < 2:\n",
        "                continue\n",
        "\n",
        "            diff = auroc_difference(baseline_boot, contrastive_boot, y_boot)\n",
        "            if not np.isnan(diff):\n",
        "                diff_stats.append(diff)\n",
        "\n",
        "        if len(diff_stats) > 0:\n",
        "            diff_stats = np.array(diff_stats)\n",
        "\n",
        "            # Compute observed difference\n",
        "            observed_diff = metrics['auroc'] - auroc_baseline\n",
        "\n",
        "            # Compute 95% CI for the difference\n",
        "            ci_low = np.percentile(diff_stats, 2.5)\n",
        "            ci_high = np.percentile(diff_stats, 97.5)\n",
        "\n",
        "            # Compute p-value (two-tailed)\n",
        "            p_value = np.mean(diff_stats <= 0)\n",
        "            if p_value > 0.5:\n",
        "                p_value = 1 - p_value\n",
        "            p_value = 2 * p_value\n",
        "\n",
        "            logger.info(f\"\\nBaseline AUROC:     {auroc_baseline:.4f}\")\n",
        "            logger.info(f\"Contrastive AUROC:  {metrics['auroc']:.4f}\")\n",
        "            logger.info(f\"\\nObserved difference: {observed_diff:.4f}\")\n",
        "            logger.info(f\"Bootstrap 95% CI:    [{ci_low:.4f}, {ci_high:.4f}]\")\n",
        "            logger.info(f\"P-value:             {p_value:.4f}\")\n",
        "\n",
        "            if ci_low > 0:\n",
        "                logger.info(\"\\n✓ Contrastive learning is SIGNIFICANTLY better than baseline (p < 0.05)\")\n",
        "            else:\n",
        "                logger.info(\"\\n✗ No significant difference between methods (p >= 0.05)\")\n",
        "\n",
        "            logger.info(\"=\"*70)\n",
        "\n",
        "            # Save to results\n",
        "            significance_results = {\n",
        "                \"method_comparison\": \"Contrastive vs Baseline\",\n",
        "                \"baseline_auroc\": float(auroc_baseline),\n",
        "                \"contrastive_auroc\": float(metrics['auroc']),\n",
        "                \"observed_difference\": float(observed_diff),\n",
        "                \"ci_low\": float(ci_low),\n",
        "                \"ci_high\": float(ci_high),\n",
        "                \"p_value\": float(p_value),\n",
        "                \"n_bootstrap_samples\": len(diff_stats),\n",
        "                \"is_significant\": bool(ci_low > 0)\n",
        "            }\n",
        "\n",
        "            # Will be saved later in results.json\n",
        "            if 'results' not in locals():\n",
        "                results = {}\n",
        "            results[\"significance_testing\"] = significance_results\n",
        "\n",
        "        else:\n",
        "            logger.warning(\"Could not compute significance test - no valid bootstrap samples\")\n",
        "\n",
        "    # ====================================================================\n",
        "    # 11. ABLATION STUDY\n",
        "    # ====================================================================\n",
        "\n",
        "    logger.info(\"\\n\" + \"=\"*70)\n",
        "    logger.info(\"ABLATION STUDY (Feature Importance)\")\n",
        "    logger.info(\"=\"*70)\n",
        "\n",
        "    feature_groups = {\n",
        "        'ΔKS (wrong)': [0],\n",
        "        'ΔΔ KS': [1],\n",
        "        'ΔΔ MAD': [2],\n",
        "        'ΔΔ tail': [3],\n",
        "        'ΔΔ var': [4],\n",
        "        'ΔΔ skew': [5],\n",
        "        'ΔΔ range': [6],\n",
        "    }\n",
        "\n",
        "    LAST_K = CONFIG['contrastive']['last_k_layers']\n",
        "    ablation_results = {}\n",
        "\n",
        "    for name, indices in feature_groups.items():\n",
        "        # Select columns for this feature group\n",
        "        cols = []\n",
        "        for idx in indices:\n",
        "            cols.extend(range(idx * LAST_K, (idx + 1) * LAST_K))\n",
        "\n",
        "        X_train_subset = X_train[:, cols]\n",
        "        X_test_subset = X_test[:, cols]\n",
        "\n",
        "        # Train simple logistic regression probe\n",
        "        scaler_ab = StandardScaler()\n",
        "        X_train_std = scaler_ab.fit_transform(X_train_subset)\n",
        "        X_test_std = scaler_ab.transform(X_test_subset)\n",
        "\n",
        "        clf = LogisticRegression(\n",
        "            penalty='l2',\n",
        "            C=1.0,\n",
        "            solver='liblinear',\n",
        "            class_weight='balanced',\n",
        "            max_iter=500,\n",
        "            random_state=CONFIG['seed']\n",
        "        )\n",
        "\n",
        "        clf.fit(X_train_std, y_train)\n",
        "        scores_ab = clf.decision_function(X_test_std)\n",
        "\n",
        "        try:\n",
        "            auroc_ab = roc_auc_score(y_test, scores_ab)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to compute AUROC for {name}: {e}\")\n",
        "            auroc_ab = np.nan\n",
        "\n",
        "        ablation_results[name] = auroc_ab\n",
        "        logger.info(f\"  {name:20s}: AUROC = {auroc_ab:.3f}\")\n",
        "\n",
        "    # Visualize ablation results\n",
        "    if ablation_results:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        names = list(ablation_results.keys())\n",
        "        scores_list = list(ablation_results.values())\n",
        "\n",
        "        plt.barh(names, scores_list, color='steelblue', alpha=0.8)\n",
        "        plt.axvline(0.5, linestyle='--', color='black', alpha=0.3, label='Chance')\n",
        "        plt.xlabel('AUROC (Test Set)')\n",
        "        plt.title('Ablation Study: Individual Feature Group Performance')\n",
        "        plt.xlim(0.4, max(scores_list) + 0.05)\n",
        "        plt.grid(True, alpha=0.3, axis='x')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_dir / \"ablation_study.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "else:\n",
        "    logger.warning(\"\\n\" + \"=\"*70)\n",
        "    logger.warning(\"CONTRASTIVE LEARNING SKIPPED\")\n",
        "    logger.warning(\"=\"*70)\n",
        "    if not CONFIG['contrastive']['enabled']:\n",
        "        logger.warning(\"Reason: Contrastive learning is disabled in CONFIG\")\n",
        "    if len(np.unique(y_train)) < 2:\n",
        "        logger.warning(f\"Reason: Training set has only {len(np.unique(y_train))} class(es)\")\n"
      ],
      "metadata": {
        "id": "v6X_9MJGZ16S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save results\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"SAVING RESULTS\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "results = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"config\": CONFIG,\n",
        "    \"data_summary\": {\n",
        "        \"n_total\": len(items),\n",
        "        \"n_train\": len(train_items),\n",
        "        \"n_test\": len(test_items),\n",
        "        \"n_train_flips\": int(y_train.sum()),\n",
        "        \"n_test_flips\": int(y_test.sum()),\n",
        "        \"train_flip_rate\": float(y_train.mean()),\n",
        "        \"test_flip_rate\": float(y_test.mean()),\n",
        "    },\n",
        "    \"model_info\": {\n",
        "        \"name\": CONFIG['model']['name'],\n",
        "        \"n_layers\": int(n_layers),\n",
        "        \"hidden_dim\": int(hidden_dim),\n",
        "    },\n",
        "    \"baseline_classifier\": {\n",
        "        \"auroc\": float(auroc_baseline) if not np.isnan(auroc_baseline) else None,\n",
        "        \"auroc_ci_low\": float(auroc_ci_low) if not np.isnan(auroc_ci_low) else None,\n",
        "        \"auroc_ci_high\": float(auroc_ci_high) if not np.isnan(auroc_ci_high) else None,\n",
        "        \"threshold\": float(threshold),\n",
        "        \"specificity\": float(specificity),\n",
        "        \"sensitivity\": float(sensitivity),\n",
        "        \"precision\": float(precision),\n",
        "        \"f1\": float(f1),\n",
        "    },\n",
        "    \"confidence_baseline\": {\n",
        "        \"auroc\": float(auroc_conf) if not np.isnan(auroc_conf) else None,\n",
        "        \"description\": \"Answer log-probability on incorrect context\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Only add contrastive results if the block executed\n",
        "# Check if metrics exists by using try/except or checking if it's in locals()\n",
        "try:\n",
        "    if 'metrics' in locals() and metrics is not None:\n",
        "        results[\"contrastive_learning\"] = {\n",
        "            \"auroc\": float(metrics['auroc']) if not np.isnan(metrics['auroc']) else None,\n",
        "            \"auroc_ci_low\": float(metrics['auroc_ci_low']) if not np.isnan(metrics['auroc_ci_low']) else None,\n",
        "            \"auroc_ci_high\": float(metrics['auroc_ci_high']) if not np.isnan(metrics['auroc_ci_high']) else None,\n",
        "            \"improvement_over_baseline\": float(metrics['auroc'] - auroc_baseline)\n",
        "                if not np.isnan(metrics['auroc']) and not np.isnan(auroc_baseline) else None,\n",
        "        }\n",
        "\n",
        "    if 'ablation_results' in locals() and ablation_results is not None:\n",
        "        results[\"ablation_study\"] = {\n",
        "            name: float(score) if not np.isnan(score) else None\n",
        "            for name, score in ablation_results.items()\n",
        "        }\n",
        "\n",
        "    # neuron-level contrastive\n",
        "    if 'neuron_contrastive_metrics' in locals() and neuron_contrastive_metrics is not None:\n",
        "        results[\"neuron_contrastive_learning\"] = {\n",
        "            \"auroc\": float(neuron_contrastive_metrics['auroc'])\n",
        "                if not np.isnan(neuron_contrastive_metrics['auroc']) else None,\n",
        "            \"auroc_ci_low\": float(neuron_contrastive_metrics['auroc_ci_low'])\n",
        "                if not np.isnan(neuron_contrastive_metrics['auroc_ci_low']) else None,\n",
        "            \"auroc_ci_high\": float(neuron_contrastive_metrics['auroc_ci_high'])\n",
        "                if not np.isnan(neuron_contrastive_metrics['auroc_ci_high']) else None,\n",
        "            \"k_neurons\": int(len(top_neurons)),\n",
        "        }\n",
        "except NameError:\n",
        "    logger.warning(\"  Contrastive learning variables not found, skipping in results\")\n",
        "\n",
        "\n",
        "try:\n",
        "    if 'auroc_neuron' in locals():\n",
        "        results[\"neuron_level_probe\"] = {\n",
        "            \"auroc\": float(auroc_neuron),\n",
        "            \"auroc_ci_low\": float(ci_low_neuron),\n",
        "            \"auroc_ci_high\": float(ci_high_neuron),\n",
        "            \"k_neurons\": int(len(top_neurons)),\n",
        "            \"last_k_layers_searched\": int(LAST_K_NEURON),\n",
        "        }\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "with open(output_dir / \"results.json\", 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "logger.info(f\"  Results saved to: {output_dir / 'results.json'}\")"
      ],
      "metadata": {
        "id": "DoKEXV5faYEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"DETAILED ERROR ANALYSIS\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "# Prepare baseline predictions\n",
        "test_predictions = (test_scores > threshold).astype(int)\n",
        "\n",
        "# Prepare contrastive predictions if available\n",
        "if CONFIG['contrastive']['enabled'] and 'test_contrastive_scores' in locals() and test_contrastive_scores is not None:\n",
        "    test_contrastive_predictions = (test_contrastive_scores > 0).astype(int)\n",
        "    has_contrastive = True\n",
        "else:\n",
        "    test_contrastive_scores = None\n",
        "    test_contrastive_predictions = None\n",
        "    has_contrastive = False\n",
        "\n",
        "# Run error analysis\n",
        "error_analyses = run_error_analysis_on_saved_data(\n",
        "    test_items=test_items,\n",
        "    test_results=test_results,\n",
        "    y_test=y_test,\n",
        "    test_scores_baseline=test_scores,\n",
        "    test_predictions_baseline=test_predictions,\n",
        "    test_scores_contrastive=test_contrastive_scores if has_contrastive else None,\n",
        "    test_predictions_contrastive=test_contrastive_predictions if has_contrastive else None,\n",
        "    output_dir=output_dir\n",
        ")\n",
        "\n",
        "# Print summary\n",
        "logger.info(\"\\n\" + \"=\"*70)\n",
        "logger.info(\"ERROR ANALYSIS SUMMARY\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "baseline_errors = error_analyses['baseline']\n",
        "logger.info(f\"\\nBaseline Method (ΔΔ MAD):\")\n",
        "logger.info(f\"  False Positive Rate: {baseline_errors['false_positives']['rate']:.1%}\")\n",
        "logger.info(f\"  False Negative Rate: {baseline_errors['false_negatives']['rate']:.1%}\")\n",
        "logger.info(f\"  FP Count: {baseline_errors['false_positives']['count']}\")\n",
        "logger.info(f\"  FN Count: {baseline_errors['false_negatives']['count']}\")\n",
        "\n",
        "if 'contrastive' in error_analyses:\n",
        "    contrastive_errors = error_analyses['contrastive']\n",
        "    logger.info(f\"\\nContrastive Learning Method:\")\n",
        "    logger.info(f\"  False Positive Rate: {contrastive_errors['false_positives']['rate']:.1%}\")\n",
        "    logger.info(f\"  False Negative Rate: {contrastive_errors['false_negatives']['rate']:.1%}\")\n",
        "    logger.info(f\"  FP Count: {contrastive_errors['false_positives']['count']}\")\n",
        "    logger.info(f\"  FN Count: {contrastive_errors['false_negatives']['count']}\")\n",
        "\n",
        "    if 'comparison' in error_analyses:\n",
        "        comp = error_analyses['comparison']\n",
        "        logger.info(f\"\\nContrastive improvements over baseline:\")\n",
        "        logger.info(f\"  Fixed {comp['fixed_fp']} false positives\")\n",
        "        logger.info(f\"  Fixed {comp['fixed_fn']} false negatives\")\n",
        "        logger.info(f\"  Introduced {comp['contrastive_only_fp']} new false positives\")\n",
        "        logger.info(f\"  Introduced {comp['contrastive_only_fn']} new false negatives\")\n",
        "\n",
        "        net_improvement = (comp['fixed_fp'] + comp['fixed_fn']) - (comp['contrastive_only_fp'] + comp['contrastive_only_fn'])\n",
        "        logger.info(f\"  Net improvement: {net_improvement} fewer total errors\")\n",
        "\n",
        "logger.info(f\"\\nDetailed analysis saved to: {output_dir / 'error_analysis.json'}\")\n",
        "logger.info(\"=\"*70)"
      ],
      "metadata": {
        "id": "6xyC3OXva-8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(significance_results)"
      ],
      "metadata": {
        "id": "jRDG6wHpVnxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPwTbKckfNTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}